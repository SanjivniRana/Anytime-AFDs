{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_MhVcpyrBAw"
      },
      "source": [
        "**Upper Bound (QP) - 2D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "szxuoO4zb72t"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_ub_qp_2D(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "    n = len(row_sum)\n",
        "    m = len(col_sum)\n",
        "    seen_array = np.array(seen)\n",
        "\n",
        "    # optimization variables\n",
        "    matrix = cp.Variable((n, m), nonneg=True)\n",
        "\n",
        "    # precomputed row and column totals of the seen matrix\n",
        "    seen_row_sums = seen_array.sum(axis=1)\n",
        "    seen_col_sums = seen_array.sum(axis=0)\n",
        "\n",
        "    # objective: minimize sum of squares of seen + matrix\n",
        "    objective = cp.Minimize(cp.sum_squares(seen_array + matrix))\n",
        "\n",
        "    # constraints for row and column sums\n",
        "    constraints = [\n",
        "        cp.sum(matrix, axis=1) + seen_row_sums <= row_sum,\n",
        "        cp.sum(matrix, axis=0) + seen_col_sums <= col_sum\n",
        "    ]\n",
        "\n",
        "    # define solver\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve(solver=cp.SCS)\n",
        "\n",
        "    if problem.status != cp.OPTIMAL:\n",
        "        raise ValueError(\"Optimization was not successful\")\n",
        "\n",
        "    g1ErrorUB = problem.value\n",
        "    row_square = np.dot(row_sum, row_sum)\n",
        "    conflict = (row_square - g1ErrorUB) / 2\n",
        "    return conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVt0GPXrHAg"
      },
      "source": [
        "**Lower Bound (Ignore Row Sum) - 2D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "OANm_ZaAcENp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_lb_ignoreSum_2D(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "  row_sum_cur = []\n",
        "  row_max_index = []\n",
        "  remaining_row_sum = []\n",
        "\n",
        "  n = len(row_sum)\n",
        "  m = len(col_sum)\n",
        "\n",
        "  # Calculate conflict\n",
        "  xi_square = 0\n",
        "  for i in range(n):\n",
        "    row_sum_cur.append(0)\n",
        "    row_max_index.append(0)\n",
        "    remaining_row_sum.append(0)\n",
        "    for j in range(m):\n",
        "      row_sum_cur[i] += seen[i][j]\n",
        "      if seen[i][row_max_index[i]] < seen[i][j]:\n",
        "        row_max_index[i] = j\n",
        "      xi_square += seen[i][j]*seen[i][j]\n",
        "    remaining_row_sum[i] = row_sum[i] - row_sum_cur[i]\n",
        "    _max = seen[i][row_max_index[i]]\n",
        "    xi_square += remaining_row_sum[i] * remaining_row_sum[i] + 2 * _max * remaining_row_sum[i]\n",
        "  row_square = 0\n",
        "  for r_s in row_sum:\n",
        "    row_square += r_s*r_s\n",
        "  conflict = (row_square - xi_square)/2\n",
        "  return conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seeWrDvJ_kPD"
      },
      "source": [
        "**Bounds (QP) - 3D**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiTZEVwXC_8J",
        "outputId": "d3eefbfa-9c0b-4bfb-b3d8-21e6ed766600"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (12.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "JADAFGsKGH88"
      },
      "outputs": [],
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_bounds_qp_3D(\n",
        "    seen: List[List[List[int]]],\n",
        "    row_sum_1: List[int],\n",
        "    row_sum_2: List[int],\n",
        "    col_sum: List[int],\n",
        "    lb_calc: bool\n",
        ") -> int:\n",
        "\n",
        "    n1 = len(row_sum_1)\n",
        "    n2 = len(row_sum_2)\n",
        "    m = len(col_sum)\n",
        "\n",
        "    seen_np = np.array(seen)\n",
        "\n",
        "    model = gp.Model()\n",
        "    model.setParam(\"OutputFlag\", 0)\n",
        "\n",
        "    # non-negative decision variables\n",
        "    matrix_vars = model.addVars(n1, n2, m, lb=0, name=\"matrix\")\n",
        "\n",
        "    # objective function: sum over (seen + x)[i,j,k1] * (seen + x)[i,j,k2] for k1 < k2\n",
        "    objective_terms = []\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            for k1 in range(m - 1):\n",
        "                for k2 in range(k1 + 1, m):\n",
        "                    t1 = seen_np[i, j, k1] + matrix_vars[i, j, k1]\n",
        "                    t2 = seen_np[i, j, k2] + matrix_vars[i, j, k2]\n",
        "                    objective_terms.append(t1 * t2)\n",
        "\n",
        "    model.setObjective(gp.quicksum(objective_terms), gp.GRB.MINIMIZE if lb_calc else gp.GRB.MAXIMIZE)\n",
        "\n",
        "    # row sum constraints\n",
        "    for i in range(n1):\n",
        "        rhs = row_sum_1[i] - seen_np[i, :, :].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for j in range(n2) for k in range(m)) <= rhs,\n",
        "            name=f\"RowSum1_{i}\"\n",
        "        )\n",
        "\n",
        "    for j in range(n2):\n",
        "        rhs = row_sum_2[j] - seen_np[:, j, :].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for i in range(n1) for k in range(m)) <= rhs,\n",
        "            name=f\"RowSum2_{j}\"\n",
        "        )\n",
        "\n",
        "    # col sum constraints\n",
        "    for k in range(m):\n",
        "        rhs = col_sum[k] - seen_np[:, :, k].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for i in range(n1) for j in range(n2)) <= rhs,\n",
        "            name=f\"ColSum_{k}\"\n",
        "        )\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == gp.GRB.OPTIMAL:\n",
        "        return model.objVal\n",
        "    else:\n",
        "        raise ValueError(\"No optimal solution found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvMaqMym_nbT"
      },
      "source": [
        "**Bounds (QP) - 4D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "_IZqungbNZsV"
      },
      "outputs": [],
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_bounds_qp_4D(\n",
        "    seen: List[List[List[List[int]]]],\n",
        "    row_sum_1: List[int],\n",
        "    row_sum_2: List[int],\n",
        "    row_sum_3: List[int],\n",
        "    col_sum: List[int],\n",
        "    lb_calc: bool\n",
        ") -> int:\n",
        "\n",
        "    n1 = len(row_sum_1)\n",
        "    n2 = len(row_sum_2)\n",
        "    n3 = len(row_sum_3)\n",
        "    m = len(col_sum)\n",
        "\n",
        "    seen_np = np.array(seen)\n",
        "\n",
        "    model = gp.Model()\n",
        "    model.setParam(\"OutputFlag\", 0)\n",
        "\n",
        "    # decision variables: non-negative\n",
        "    matrix_vars = model.addVars(n1, n2, n3, m, lb=0, name=\"matrix\")\n",
        "\n",
        "    # objective: sum over (seen + matrix)[i,j,k,l1] * (seen + matrix)[i,j,k,l2] for l1 < l2\n",
        "    objective_terms = []\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            for k in range(n3):\n",
        "                for l1 in range(m - 1):\n",
        "                    for l2 in range(l1 + 1, m):\n",
        "                        term1 = seen_np[i, j, k, l1] + matrix_vars[i, j, k, l1]\n",
        "                        term2 = seen_np[i, j, k, l2] + matrix_vars[i, j, k, l2]\n",
        "                        objective_terms.append(term1 * term2)\n",
        "\n",
        "    if lb_calc:\n",
        "        model.setObjective(gp.quicksum(objective_terms), gp.GRB.MINIMIZE)\n",
        "    else:\n",
        "        model.setObjective(gp.quicksum(objective_terms), gp.GRB.MAXIMIZE)\n",
        "\n",
        "    # Constraints\n",
        "\n",
        "    # 1D row sums\n",
        "    for i in range(n1):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for j in range(n2) for k in range(n3) for l in range(m))\n",
        "        rhs = row_sum_1[i] - seen_np[i, :, :, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum1_{i}\")\n",
        "\n",
        "    # 2D row sums\n",
        "    for j in range(n2):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for k in range(n3) for l in range(m))\n",
        "        rhs = row_sum_2[j] - seen_np[:, j, :, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum2_{j}\")\n",
        "\n",
        "    # 3D row sums\n",
        "    for k in range(n3):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for j in range(n2) for l in range(m))\n",
        "        rhs = row_sum_3[k] - seen_np[:, :, k, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum3_{k}\")\n",
        "\n",
        "    # Column sums\n",
        "    for l in range(m):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for j in range(n2) for k in range(n3))\n",
        "        rhs = col_sum[l] - seen_np[:, :, :, l].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"ColSum_{l}\")\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == gp.GRB.OPTIMAL:\n",
        "        return model.objVal\n",
        "    else:\n",
        "        raise ValueError(\"No optimal solution found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mjiMR9dkVHz"
      },
      "source": [
        "**Approx Top-K Ranking (Sampling/Mean)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "8ZLEkZhlAgx_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Approach 1 : Using mean to find top-k\n",
        "def cal_mean(attributes, ubDict, lbDict, e):\n",
        "\n",
        "  meanTopk = []\n",
        "  emax_attr = []\n",
        "\n",
        "  for attribute in attributes:\n",
        "    lowerB = lbDict[attribute]\n",
        "    upperB = ubDict[attribute]\n",
        "\n",
        "    if upperB <= e:\n",
        "        emax_attr.append(attribute)\n",
        "\n",
        "  for attr in emax_attr:\n",
        "    lb = lbDict[attr]\n",
        "    ub = ubDict[attr]\n",
        "    meanVal = (lb+ub)/2\n",
        "    meanTopk.append((attr, meanVal))\n",
        "\n",
        "  meanTopk = sorted(meanTopk, key=lambda x: x[1])\n",
        "\n",
        "  meanTopk = [attr[0] for attr in meanTopk]\n",
        "\n",
        "  return meanTopk\n",
        "\n",
        "# Approach 2 : Using sampling to find top-k\n",
        "def cal_sample_prob(attributes, ubDict, lbDict, e):\n",
        "\n",
        "  emax_attr = []\n",
        "  order_counts = defaultdict(int)\n",
        "\n",
        "  for attribute in attributes:\n",
        "    lowerB = lbDict[attribute]\n",
        "    upperB = ubDict[attribute]\n",
        "\n",
        "    if upperB <= e:\n",
        "      emax_attr.append(attribute)\n",
        "\n",
        "  for _ in range(1000):\n",
        "    samples = {}\n",
        "    for attr in emax_attr:\n",
        "      lower_bound = lbDict[attr]\n",
        "      upper_bound = ubDict[attr]\n",
        "      samples[str(attr)] = random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "    sorted_order = tuple(sorted(samples, key=samples.get))\n",
        "    order_counts[sorted_order] += 1\n",
        "\n",
        "  most_frequent_order = max(order_counts, key=order_counts.get)\n",
        "\n",
        "  return most_frequent_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMTcJ1qDsO_X"
      },
      "source": [
        "**Actual Top-K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "J05xQ4sSN0I0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "\n",
        "def calculateActualTopk(file_name, e):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  actualConflictDict_4D = {}\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "    g1Error_4D = 0\n",
        "\n",
        "    for matrix_3d in matrix_3lhs:\n",
        "      for matrix_2d in matrix_3d:\n",
        "          for row in matrix_2d:\n",
        "              pairs = itertools.combinations(row, 2)\n",
        "              for pair in pairs:\n",
        "                  g1Error_4D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_4D[str(A_name)] = g1Error_4D/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  actualConflictDict_3D = {}\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "    g1Error_3D = 0\n",
        "\n",
        "    for matrix_2d in matrix_2lhs:\n",
        "      for row in matrix_2d:\n",
        "          pairs = itertools.combinations(row, 2)\n",
        "          for pair in pairs:\n",
        "              g1Error_3D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_3D[str(A_name)] = g1Error_3D/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  actualConflictDict_2D = {}\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "    g1Error_2D = 0\n",
        "    for row in matrix:\n",
        "      pairs = itertools.combinations(row, 2)\n",
        "      for pair in pairs:\n",
        "        g1Error_2D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_2D[A_name] = g1Error_2D/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  mergedConflictDict = {**actualConflictDict_2D, **actualConflictDict_3D, **actualConflictDict_4D}\n",
        "  unsortedConflictDict = {k: v for k, v in mergedConflictDict.items() if v <= e}\n",
        "  sortedConflictDict = {k: v for k, v in sorted(mergedConflictDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "  actualTopk = sorted(sortedConflictDict, key=lambda k: sortedConflictDict[k])\n",
        "\n",
        "  return sortedConflictDict, unsortedConflictDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3xWXzXV_5uz"
      },
      "source": [
        "**LPDS - Uninformed Prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "k5N-mBOcVkzq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def removeSingleMarginals(calc_matrix, row_sum, col_sum):\n",
        "\n",
        "  row_sum_copy = row_sum[:]\n",
        "  col_sum_copy = col_sum[:]\n",
        "  matrix_copy = [row[:] for row in calc_matrix]\n",
        "\n",
        "  rows_to_remove = [i for i, rsum in enumerate(row_sum_copy) if rsum == 1]\n",
        "\n",
        "  for i in sorted(rows_to_remove, reverse=True):\n",
        "        row = matrix_copy[i]\n",
        "        for j, val in enumerate(row):\n",
        "            col_sum_copy[j] -= val\n",
        "\n",
        "        del matrix_copy[i]\n",
        "        del row_sum_copy[i]\n",
        "\n",
        "  return matrix_copy, row_sum_copy, col_sum_copy\n",
        "\n",
        "def calculateBoundsLPDS(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainUBDict = {}\n",
        "  mainLBDict = {}\n",
        "\n",
        "  boundsTime2D = {}\n",
        "  boundsTime3D = {}\n",
        "  boundsTime4D = {}\n",
        "\n",
        "  ub2TimeDict = {}\n",
        "  ub3TimeDict = {}\n",
        "  ub4TimeDict = {}\n",
        "  lb2TimeDict = {}\n",
        "  lb3TimeDict = {}\n",
        "  lb4TimeDict = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    op1 = []\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainUBDict:\n",
        "            mainUBDict[index] = {}\n",
        "\n",
        "          if index not in mainLBDict:\n",
        "            mainLBDict[index] = {}\n",
        "\n",
        "          startTime4D = time.time()\n",
        "\n",
        "          startUBTime4D = time.time()\n",
        "          ubValue = g1_bounds_qp_4D(matrix_3lhs, rowSum_a1, rowSum_a2, rowSum_a3, colSum, False)\n",
        "          stopUBTime4D = time.time()\n",
        "\n",
        "          startLBTime4D = time.time()\n",
        "          lbValue = g1_bounds_qp_4D(matrix_3lhs, rowSum_a1, rowSum_a2, rowSum_a3, colSum, True)\n",
        "          stopLBTime4D = time.time()\n",
        "\n",
        "          stopTime4D = time.time()\n",
        "\n",
        "          boundsTime4D[index] = stopTime4D - startTime4D\n",
        "\n",
        "          ub4TimeDict[index] = stopUBTime4D - startUBTime4D\n",
        "          lb4TimeDict[index] = stopLBTime4D - startLBTime4D\n",
        "\n",
        "          ubValue = ubValue/norm\n",
        "          lbValue = lbValue/norm\n",
        "\n",
        "          if not op1:\n",
        "            op1.append([index+1, ubValue, lbValue])\n",
        "            u1 = ubValue\n",
        "            l2 = lbValue\n",
        "          else:\n",
        "            op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "            u1 = min(ubValue, op1[-1][1])\n",
        "            l2 = max(lbValue, op1[-1][2])\n",
        "\n",
        "          mainUBDict[index][A_name] = u1\n",
        "          mainLBDict[index][A_name] = l2\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    op1 = []\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          startTime3D = time.time()\n",
        "\n",
        "          startUBTime3D = time.time()\n",
        "          ubValue = g1_bounds_qp_3D(matrix_2lhs, rowSum_a1, rowSum_a2, colSum, False)\n",
        "          stopUBTime3D = time.time()\n",
        "\n",
        "          startLBTime3D = time.time()\n",
        "          lbValue = g1_bounds_qp_3D(matrix_2lhs, rowSum_a1, rowSum_a2, colSum, True)\n",
        "          stopLBTime3D = time.time()\n",
        "\n",
        "          stopTime3D = time.time()\n",
        "\n",
        "          boundsTime3D[index] = stopTime3D - startTime3D\n",
        "\n",
        "          ub3TimeDict[index] = stopUBTime3D - startUBTime3D\n",
        "          lb3TimeDict[index] = stopLBTime3D - startLBTime3D\n",
        "\n",
        "          ubValue = ubValue/norm\n",
        "          lbValue = lbValue/norm\n",
        "\n",
        "          if not op1:\n",
        "            op1.append([index+1, ubValue, lbValue])\n",
        "            u1 = ubValue\n",
        "            l2 = lbValue\n",
        "          else:\n",
        "            op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "            u1 = min(ubValue, op1[-1][1])\n",
        "            l2 = max(lbValue, op1[-1][2])\n",
        "\n",
        "          mainUBDict[index][A_name] = u1\n",
        "          mainLBDict[index][A_name] = l2\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    op1 = []\n",
        "    calc_matrix = []\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        mat, rowS, colS = removeSingleMarginals(calc_matrix, row_sum, col_sum)\n",
        "\n",
        "        startTime2D = time.time()\n",
        "\n",
        "        startUBTime2D = time.time()\n",
        "        ubValue = g1_ub_qp_2D(mat, rowS, colS)\n",
        "        stopUBTime2D = time.time()\n",
        "\n",
        "        startLBTime2D = time.time()\n",
        "        lbValue = g1_lb_ignoreSum_2D(mat, rowS, colS)\n",
        "        stopLBTime2D = time.time()\n",
        "\n",
        "        stopTime2D = time.time()\n",
        "\n",
        "        ub2TimeDict[index] = stopUBTime2D - startUBTime2D\n",
        "        lb2TimeDict[index] = stopLBTime2D - startLBTime2D\n",
        "\n",
        "        boundsTime2D[index] = stopTime2D - startTime2D\n",
        "\n",
        "        ubValue = ubValue/norm\n",
        "        lbValue = lbValue/norm\n",
        "\n",
        "        if not op1:\n",
        "          op1.append([index+1, ubValue, lbValue])\n",
        "          u1 = ubValue\n",
        "          l1 = lbValue\n",
        "\n",
        "        else:\n",
        "          op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "          u1 = min(ubValue, op1[-1][1])\n",
        "          l1 = max(lbValue, op1[-1][2])\n",
        "\n",
        "        mainUBDict[index][A_name] = u1\n",
        "        mainLBDict[index][A_name] = l1\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  indexTotalTimeDict = {key: boundsTime2D.get(key, 0) + boundsTime3D.get(key, 0) + boundsTime4D.get(key, 0)\n",
        "               for key in set(boundsTime2D) | set(boundsTime3D) | set(boundsTime4D)}\n",
        "\n",
        "  indexTotalUBTimeDict = {key: ub2TimeDict.get(key, 0) + ub3TimeDict.get(key, 0) + ub4TimeDict.get(key, 0)\n",
        "               for key in set(ub2TimeDict) | set(ub3TimeDict) | set(ub4TimeDict)}\n",
        "\n",
        "  indexTotalLBTimeDict = {key: lb2TimeDict.get(key, 0) + lb3TimeDict.get(key, 0) + lb4TimeDict.get(key, 0)\n",
        "               for key in set(lb2TimeDict) | set(lb3TimeDict) | set(lb4TimeDict)}\n",
        "\n",
        "  df1 = pd.DataFrame(list(boundsTime2D.items()), columns=['Index', 'Time'])\n",
        "  df2 = pd.DataFrame(list(boundsTime3D.items()), columns=['Index', 'Time'])\n",
        "  df3 = pd.DataFrame(list(boundsTime4D.items()), columns=['Index', 'Time'])\n",
        "  df4 = pd.DataFrame(list(indexTotalTimeDict.items()), columns=['Index', 'Time'])\n",
        "  df5 = pd.DataFrame(list(indexTotalUBTimeDict.items()), columns=['Index', 'Time'])\n",
        "  df6 = pd.DataFrame(list(indexTotalLBTimeDict.items()), columns=['Index', 'Time'])\n",
        "\n",
        "  df1.to_excel('2DTime.xlsx', index=False)\n",
        "  df2.to_excel('3DTime.xlsx', index=False)\n",
        "  df3.to_excel('4DTime.xlsx', index=False)\n",
        "  df4.to_excel('TotalTimeLP.xlsx', index=False)\n",
        "  df5.to_excel('AblationStudyUB.xlsx', index=False)\n",
        "  df6.to_excel('AblationStudyLB.xlsx', index=False)\n",
        "\n",
        "  return mainUBDict, mainLBDict, indexTotalTimeDict, boundsTime2D, boundsTime3D, boundsTime4D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIJkYq-a9hFn"
      },
      "source": [
        "**IPF - Informed Prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYxn_M_i9ox2",
        "outputId": "8406ce88-e3b1-4a04-8b72-7b4216a5871e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipfn in /usr/local/lib/python3.11/dist-packages (1.4.4)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from ipfn) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipfn) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.22.0->ipfn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipfn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipfn import ipfn\n",
        "\n",
        "def calError_2D(matrix):\n",
        "  error = 0\n",
        "  for row in matrix:\n",
        "    pairs = itertools.combinations(row, 2)\n",
        "    for pair in pairs:\n",
        "      error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calError_3D(matrix):\n",
        "  error = 0\n",
        "  for matrix_2d in matrix:\n",
        "    for row in matrix_2d:\n",
        "      pairs = itertools.combinations(row, 2)\n",
        "      for pair in pairs:\n",
        "          error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calError_4D(matrix):\n",
        "  error = 0\n",
        "  for matrix_3d in matrix:\n",
        "    for matrix_2d in matrix_3d:\n",
        "        for row in matrix_2d:\n",
        "            pairs = itertools.combinations(row, 2)\n",
        "            for pair in pairs:\n",
        "                error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calculateIPF(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainIPFDict = {}\n",
        "  boundsTimeIPF2D = {}\n",
        "  boundsTimeIPF3D = {}\n",
        "  boundsTimeIPF4D = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    dimensions_4D = [[0], [1], [2], [3]]\n",
        "    constraints_4D = [rowSum_a1, rowSum_a2, rowSum_a3, colSum]\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainIPFDict:\n",
        "            mainIPFDict[index] = {}\n",
        "\n",
        "          matrix_3lhs_np = np.array(matrix_3lhs)\n",
        "          startTimeIPF4D = time.time()\n",
        "          ipfMatrix_4D = ipfn.ipfn(matrix_3lhs_np, constraints_4D, dimensions_4D)\n",
        "          matrix_3lhs_ipf = np.round(ipfMatrix_4D.iteration()).astype(int)\n",
        "          error = calError_4D(matrix_3lhs_ipf)\n",
        "          stopTimeIPF4D = time.time()\n",
        "\n",
        "          boundsTimeIPF4D[index] = stopTimeIPF4D - startTimeIPF4D\n",
        "          mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    dimensions_3D = [[0], [1], [2]]\n",
        "    constraints_3D = [rowSum_a1, rowSum_a2, colSum]\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          matrix_2lhs_np = np.array(matrix_2lhs)\n",
        "          startTimeIPF3D = time.time()\n",
        "          ipfMatrix_3D = ipfn.ipfn(matrix_2lhs_np, constraints_3D, dimensions_3D)\n",
        "          matrix_2lhs_ipf = np.round(ipfMatrix_3D.iteration()).astype(int)\n",
        "          error = calError_3D(matrix_2lhs_ipf)\n",
        "          stopTimeIPF3D = time.time()\n",
        "\n",
        "          boundsTimeIPF3D[index] = stopTimeIPF3D - startTimeIPF3D\n",
        "          mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    calc_matrix = []\n",
        "    dimensions_2D = [[0], [1]]\n",
        "    constraints_2D = [row_sum, col_sum]\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        calc_matrix_np = np.array(calc_matrix)\n",
        "        startTimeIPF2D = time.time()\n",
        "        ipfMatrix_2D = ipfn.ipfn(calc_matrix_np, constraints_2D, dimensions_2D)\n",
        "        calc_matrix_ipf = np.round(ipfMatrix_2D.iteration()).astype(int)\n",
        "        error = calError_2D(calc_matrix_ipf)\n",
        "        stopTimeIPF2D = time.time()\n",
        "\n",
        "        boundsTimeIPF2D[index] = stopTimeIPF2D - startTimeIPF2D\n",
        "        mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  indexTotalTimeDictIPF = {key: boundsTimeIPF2D.get(key, 0) + boundsTimeIPF3D.get(key, 0) + boundsTimeIPF4D.get(key, 0)\n",
        "               for key in set(boundsTimeIPF2D) | set(boundsTimeIPF3D) | set(boundsTimeIPF4D)}\n",
        "\n",
        "  return mainIPFDict, indexTotalTimeDictIPF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce6SldH9awn"
      },
      "source": [
        "**TANE/Greedy - Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "FMIRD2EI9gQR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculateBaseline(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainBLDict = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainBLDict:\n",
        "            mainBLDict[index] = {}\n",
        "\n",
        "          g1ErrorBL_4D = 0\n",
        "\n",
        "          for matrix_3d in matrix_3lhs:\n",
        "            for matrix_2d in matrix_3d:\n",
        "                for row in matrix_2d:\n",
        "                    pairs = itertools.combinations(row, 2)\n",
        "                    for pair in pairs:\n",
        "                        g1ErrorBL_4D += pair[0] * pair[1]\n",
        "\n",
        "          mainBLDict[index][str(A_name)] = g1ErrorBL_4D/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          g1ErrorBL_3D = 0\n",
        "\n",
        "          for matrix_2d in matrix_2lhs:\n",
        "            for row in matrix_2d:\n",
        "                pairs = itertools.combinations(row, 2)\n",
        "                for pair in pairs:\n",
        "                    g1ErrorBL_3D += pair[0] * pair[1]\n",
        "\n",
        "          mainBLDict[index][str(A_name)] = g1ErrorBL_3D/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        g1ErrorBL_2D = 0\n",
        "        for row in matrix:\n",
        "          pairs = itertools.combinations(row, 2)\n",
        "          for pair in pairs:\n",
        "            g1ErrorBL_2D += pair[0] * pair[1]\n",
        "\n",
        "        mainBLDict[index][str(A_name)] = g1ErrorBL_2D/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  return mainBLDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eu29HSKAZXM"
      },
      "source": [
        "**Pyro++ - SOTA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "7amd1eUF38e1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "def log_to_dataframe(log_file):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  with open(log_file, 'r') as log:\n",
        "    for line in log:\n",
        "      if \"LHS:\" in line and \"RHS:\" in line and \"Error:\" in line:\n",
        "        try:\n",
        "          lhs = line.split('LHS:')[1].split('RHS:')[0].strip().strip('[]').replace(']', '').replace(',', '').strip()\n",
        "          rhs = line.split('RHS:')[1].split('Error:')[0].strip().strip('[]').replace(']', '').replace(',', '').strip()\n",
        "          error = float(line.split('Error:')[1].strip())\n",
        "\n",
        "          data.append([lhs, rhs, error])\n",
        "        except IndexError as e:\n",
        "          print(f\"Skipping line due to error: {e}\")\n",
        "          continue\n",
        "\n",
        "  sorted_data = sorted(data, key=lambda x: x[2])\n",
        "  df = pd.DataFrame(sorted_data, columns=['LHS', 'RHS', 'Error'])\n",
        "\n",
        "  return df\n",
        "\n",
        "def generate_supersets(sets_str):\n",
        "\n",
        "  supersets = []\n",
        "  sets = [set(map(int, s)) for s in sets_str]\n",
        "  unique_elements = set().union(*sets)\n",
        "\n",
        "  for r in range(1, 4):\n",
        "      for combo in combinations(unique_elements, r):\n",
        "          superset = set(combo)\n",
        "          if any(s.issubset(superset) for s in sets):\n",
        "              supersets.append(superset)\n",
        "\n",
        "  supersets = sorted(supersets, key=lambda x: (len(x), sorted(x)))\n",
        "\n",
        "  supersets_str = [''.join(sorted(map(str, superset))) for superset in supersets]\n",
        "\n",
        "  return supersets_str\n",
        "\n",
        "def calculatePyroPrecision(filename, actualTopk, actualG1Dict):\n",
        "\n",
        "  df_og = pd.read_csv(filename, header = 0)\n",
        "  pdbxCol = df_og.columns.tolist()\n",
        "  numCol = df_og.shape[1]\n",
        "\n",
        "  pyroplusTopk = []\n",
        "  patkGraph_pyro = []\n",
        "  ndcgatkGraph_pyro = []\n",
        "  folder_path = './' + 'logFiles'\n",
        "\n",
        "  for fn in sorted(os.listdir(folder_path)):\n",
        "\n",
        "    # note: only pass files upto the percent of data read (60% for our experiments), pass in correct order\n",
        "    log_file_path = os.path.join(folder_path, fn)\n",
        "    df_pyro = log_to_dataframe(log_file_path)\n",
        "    pyroTopK = []\n",
        "\n",
        "    for index, row in df_pyro.iterrows():\n",
        "      col1 = row['LHS']\n",
        "      col2 = row['RHS']\n",
        "      if col2[6:] == str(numCol):\n",
        "        col = re.sub(r'[^0-9]', '', col1)\n",
        "        pyroTopK.append(col)\n",
        "\n",
        "    pyroplusDictSorted = {}\n",
        "    pyroplusDictUnsorted = {}\n",
        "\n",
        "    if pyroTopK:\n",
        "\n",
        "      pyroplusList = generate_supersets(pyroTopK)\n",
        "      pyroplusDictUnsorted = {ky: v for ky, v in actualG1Dict.items() if ky in pyroplusList}\n",
        "      pyroplusDictSorted = dict(sorted(pyroplusDictUnsorted.items(), key=lambda item: item[1]))\n",
        "      pyroplusTopk = sorted(pyroplusDictSorted, key=lambda ky: pyroplusDictSorted[ky])\n",
        "\n",
        "      patkGraph_pyro.append(calcPrecision(actualTopk, pyroplusTopk))\n",
        "\n",
        "    else:\n",
        "      patkGraph_pyro.append(0.0)\n",
        "\n",
        "  return patkGraph_pyro, pyroplusTopk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nv40NLkrwdb"
      },
      "source": [
        "**Score Calculation - Precision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "UHoXOPNIa2rp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def excelToDict(file_path):\n",
        "\n",
        "  df = pd.read_excel(file_path)\n",
        "  data_dict = {}\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    index_value = row['Index']\n",
        "    sub_dict = {column: row[column] for column in df.columns if column != 'Index'}\n",
        "    data_dict[index_value] = sub_dict\n",
        "\n",
        "  return data_dict\n",
        "\n",
        "def calcPrecision(y_true, y_pred):\n",
        "\n",
        "  y_true_set = set(y_true)\n",
        "  y_pred_set = set(y_pred)\n",
        "\n",
        "  true_positives = y_true_set.intersection(y_pred_set)\n",
        "  precision = len(true_positives) / len(y_pred_set) if len(y_pred_set) > 0 else 0.0\n",
        "\n",
        "  return precision\n",
        "\n",
        "def calcRecall(y_true, y_pred):\n",
        "\n",
        "    y_true_set = set(y_true)\n",
        "    y_pred_set = set(y_pred)\n",
        "\n",
        "    true_positives = y_true_set.intersection(y_pred_set)\n",
        "    recall = len(true_positives) / len(y_true_set) if len(y_true_set) > 0 else 0.0\n",
        "\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aen2MVsRAky7"
      },
      "source": [
        "**Generate Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "e1xSiwPGsVsl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def calcPrecisionForEpsilonAFD(file_name, dataInterval, rows, e, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  prec_lpds, prec_bl, prec_ipf, prec_pyro = [], [], [], []\n",
        "  rec_lpds, rec_bl, rec_ipf, rec_pyro = [], [], [], []\n",
        "\n",
        "  # actual top-k\n",
        "  actualTopkDict, actualDict = calculateActualTopk(file_name, e)\n",
        "  actualTopk = sorted(actualTopkDict, key=lambda ky: actualTopkDict[ky])\n",
        "\n",
        "  # approx algorithms\n",
        "  calculateBoundsLPDS(file_name, dataInterval, stopAtIndex)\n",
        "  calculateBaseline(file_name, dataInterval, stopAtIndex)\n",
        "  calculateIPF(file_name, dataInterval, stopAtIndex)\n",
        "  prec_pyro, approxTopk_pyroplus = calculatePyroPrecision(file_name, actualTopk, actualDict)\n",
        "\n",
        "  ubDict = excelToDict(UBResult)\n",
        "  lbDict = excelToDict(LBResult)\n",
        "  baselineDict = excelToDict(BaselineResult)\n",
        "  ipfDict = excelToDict(IPFResult)\n",
        "\n",
        "  keys = ubDict.keys()\n",
        "\n",
        "  for key in keys:\n",
        "\n",
        "    ubdataDict = ubDict[key]\n",
        "    lbdataDict = lbDict[key]\n",
        "    attr = list(ubdataDict.keys())\n",
        "    baselinedataDict = baselineDict[key]\n",
        "    ipfdataDict = ipfDict[key]\n",
        "\n",
        "    approxTopk_lpds = cal_mean(attr, ubdataDict, lbdataDict, e)\n",
        "\n",
        "    blTopkDict = {k: v for k, v in sorted(baselinedataDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "    approxTopk_bl = sorted(blTopkDict, key=lambda y: blTopkDict[y])\n",
        "\n",
        "    ipfTopkDict = {k: v for k, v in sorted(ipfdataDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "    approxTopk_ipf = sorted(ipfTopkDict, key=lambda x: ipfTopkDict[x])\n",
        "\n",
        "    prec_lpds.append(calcPrecision(actualTopk, approxTopk_lpds))\n",
        "    prec_bl.append(calcPrecision(actualTopk, approxTopk_bl))\n",
        "    prec_ipf.append(calcPrecision(actualTopk, approxTopk_ipf))\n",
        "\n",
        "    rec_lpds.append(calcRecall(actualTopk, approxTopk_lpds))\n",
        "    rec_bl.append(calcRecall(actualTopk, approxTopk_bl))\n",
        "    rec_ipf.append(calcRecall(actualTopk, approxTopk_ipf))\n",
        "\n",
        "  generatePrecisionGraph(prec_lpds, prec_ipf, prec_bl, prec_pyro, rows, dataInterval, e, stopAtIndex, fn, sortedStr)\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y95GD2Fplnw"
      },
      "source": [
        "**Precision Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "_6xtS_sM3Sih"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generatePrecisionGraph(prec_lpds, prec_ipf, prec_bl, prec_pyro, rows, dataInterval, e, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bar_width = 0.15\n",
        "  index = np.arange(len(rows_read))\n",
        "\n",
        "  patterns = ['\\\\', '.', '-', 'x']\n",
        "\n",
        "  plt.bar(index, prec_lpds, bar_width, label='AppxLP', hatch=patterns[0], facecolor='skyblue', edgecolor='black')\n",
        "  plt.bar(index + bar_width, prec_ipf, bar_width, label='AppxIPF', hatch=patterns[1], facecolor='khaki', edgecolor='black')\n",
        "  plt.bar(index + 2 * bar_width, prec_bl, bar_width, label='TANE', hatch=patterns[2], facecolor='darkseagreen', edgecolor='black')\n",
        "  plt.bar(index + 3 * bar_width, prec_pyro, bar_width, label='Pyro++', hatch=patterns[3], facecolor='lightsalmon', edgecolor='black')\n",
        "\n",
        "  plt.xlabel('Percent of Data Read', fontsize=16, fontweight='bold')\n",
        "  plt.ylabel('Precision', fontsize=16, fontweight='bold')\n",
        "\n",
        "  plt.xticks(index + bar_width * 1.75, [f'{round(r, 0)}' for r in rows_read], fontsize=16)\n",
        "  plt.yticks(fontsize=16)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.22), ncol=2, prop={'weight': 'bold', 'size': 14})\n",
        "\n",
        "  plt.savefig('Precision_e='+str(e)+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL9CsUg7BO2M"
      },
      "source": [
        "**Dataset Sorting (Key/Non-Key)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "JPILjgoQCUFq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sortDataset(file_name, attrList, attr, sortOrder):\n",
        "\n",
        "  if attr != '0':\n",
        "    df = pd.read_csv(file_name)\n",
        "    df_sorted = df.sort_values(by=attrList, ascending=sortOrder)\n",
        "    sortedFN = 'Sorted_X'+attr+'_'+file_name\n",
        "    df_sorted.to_csv(sortedFN, index=False)\n",
        "  else:\n",
        "    return file_name\n",
        "\n",
        "  return sortedFN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk-h_e7Kpq6l"
      },
      "source": [
        "**Start Point** (Define parameters and run code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "collapsed": true,
        "id": "mV8VPV5QBYKX"
      },
      "outputs": [],
      "source": [
        "def getDatasetParam(datasetName):\n",
        "\n",
        "    dataset_config = {\n",
        "        'dataset1': {'fn': 'DB Status', 'rows': 10000, 'interval': 1000, 'fileName': 'DBStatus.csv'},\n",
        "        'dataset2': {'fn': 'Letter', 'rows': 20000, 'interval': 2000, 'fileName': 'Letter.csv'},\n",
        "        'dataset3': {'fn': 'Student', 'rows': 1000, 'interval': 100, 'fileName': 'Student.csv'},\n",
        "        'dataset4': {'fn': 'Synthetic1', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData1.csv'},\n",
        "        'dataset5': {'fn': 'Synthetic2', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData2.csv'},\n",
        "        'dataset6': {'fn': 'Synthetic3', 'rows': 160000, 'interval': 16000, 'fileName': 'SyntheticData3.csv'},\n",
        "        'dataset7': {'fn': 'Synthetic4', 'rows': 48000, 'interval': 4800, 'fileName': 'SyntheticData4.csv'}\n",
        "    }\n",
        "\n",
        "    if datasetName in dataset_config:\n",
        "        return dataset_config[datasetName]\n",
        "    else:\n",
        "        raise ValueError(f\"{datasetName} not found.\")\n",
        "\n",
        "def AFDStartPoint():\n",
        "\n",
        "  # define dataset paramters\n",
        "  datasetNum = 'dataset3'\n",
        "  params = getDatasetParam(datasetNum)\n",
        "  fn = params['fn']\n",
        "  rows = params['rows']\n",
        "  interval = params['interval']\n",
        "  fileName = params['fileName']\n",
        "\n",
        "  # define sorting order\n",
        "  attr = '124'\n",
        "  attrList = ['1', '2', '4']\n",
        "  sortOrder = [True, True, True]\n",
        "  sortedStr = '(Sorted By Key Attribute)'\n",
        "  data = sortDataset(fileName, attrList, attr, sortOrder)\n",
        "\n",
        "  # threshold used by pyro\n",
        "  e = 0.01\n",
        "  # define limit of records read\n",
        "  stopAtIndex = 0.6*rows\n",
        "\n",
        "  # pass the parameters and generate results\n",
        "  calcPrecisionForEpsilonAFD(data, interval, rows, e, stopAtIndex, fn, sortedStr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuE0dEd3glD0",
        "outputId": "32f5b139-e05b-469d-e754-bdf2c411d1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4D Actual Conflict {'123': 0.001016016016016016, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '234': 0.0012242242242242241}\n",
            "3D Actual Conflict {'12': 0.0024564564564564565, '13': 0.003051051051051051, '14': 0.004741741741741742, '23': 0.001990990990990991, '24': 0.003058058058058058, '34': 0.003792792792792793}\n",
            "2D Actual Conflict {'1': 0.0074444444444444445, '2': 0.00487987987987988, '3': 0.006048048048048048, '4': 0.009414414414414414}\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "./logFiles/a_Extracted_PDBX_10000_first_row.log\n",
            "col \n",
            "./logFiles/b_extracted_PDBX_10000_10percent.log\n",
            "col 3\n",
            "col 4\n",
            "./logFiles/c_extracted_PDBX_10000_20percent.log\n",
            "col 3\n",
            "col 4\n",
            "./logFiles/d_extracted_PDBX_10000_30percent.log\n",
            "col 46\n",
            "col 3\n",
            "./logFiles/e_extracted_PDBX_10000_40percent.log\n",
            "col 46\n",
            "col 3\n",
            "./logFiles/f_extracted_PDBX_10000_50percent.log\n",
            "col 46\n",
            "col 46\n",
            "col 3\n",
            "./logFiles/g_extracted_PDBX_10000_60percent.log\n",
            "col 46\n",
            "col 46\n",
            "col 3\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['123', '234', '124', '24', '23', '12', '134', '13', '14', '34']\n",
            "approxTopk_ipf ['123', '124', '134', '234', '12', '13', '14', '23', '24', '34', '1', '2', '3', '4']\n",
            "approxTopk_bl ['123', '124', '134', '234', '12', '13', '14', '23', '24', '34', '1', '2', '3', '4']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.005055091438176829), '124': np.float64(0.005055165564079865), '134': np.float64(0.007552552552552553), '234': np.float64(0.005055159921432523), '12': np.float64(0.005055180298748378), '13': np.float64(0.007552552552552553), '14': np.float64(0.007552552552552553), '23': np.float64(0.005055179648339226), '24': np.float64(0.005055171131689207), '34': np.float64(0.009414414414414414), '1': np.float64(0.2505740740740741), '2': np.float64(0.1670975975975976), '3': np.float64(0.2712957957957958), '4': np.float64(0.2704339339339339)}\n",
            "lbdataDict {'123': np.float64(0.0), '124': np.float64(0.0), '134': np.float64(0.0), '234': np.float64(0.0), '12': np.float64(0.0), '13': np.float64(0.0), '14': np.float64(0.0), '23': np.float64(0.0), '24': np.float64(0.0), '34': np.float64(0.0), '1': np.float64(0.0), '2': np.float64(0.0), '3': np.float64(0.0), '4': np.float64(0.0)}\n",
            "ipfTopkDict {'123': np.float64(0.0), '124': np.float64(0.0), '134': np.float64(0.0), '234': np.float64(0.0), '12': np.float64(0.0), '13': np.float64(0.0), '14': np.float64(0.0), '23': np.float64(0.0), '24': np.float64(0.0), '34': np.float64(0.0), '1': np.float64(0.0), '2': np.float64(0.0), '3': np.float64(0.0), '4': np.float64(0.0)}\n",
            "blTopkDict {'123': np.float64(0.0), '124': np.float64(0.0), '134': np.float64(0.0), '234': np.float64(0.0), '12': np.float64(0.0), '13': np.float64(0.0), '14': np.float64(0.0), '23': np.float64(0.0), '24': np.float64(0.0), '34': np.float64(0.0), '1': np.float64(0.0), '2': np.float64(0.0), '3': np.float64(0.0), '4': np.float64(0.0)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['123', '234', '23', '124', '12', '24', '134', '13', '14', '34']\n",
            "approxTopk_ipf ['123', '134', '234', '13', '23', '34', '3']\n",
            "approxTopk_bl ['123', '134', '234', '13', '23', '34', '3', '124', '12', '14', '24', '1', '2', '4']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.004695694886069507), '124': np.float64(0.005055165564079865), '134': np.float64(0.006291291291291292), '234': np.float64(0.004695695695695696), '12': np.float64(0.00505517066521062), '13': np.float64(0.006809611005314461), '14': np.float64(0.007552552552552553), '23': np.float64(0.004695695765088739), '24': np.float64(0.005055171131689207), '34': np.float64(0.007773773773773774), '1': np.float64(0.2456671669432517), '2': np.float64(0.1621906906740826), '3': np.float64(0.2686311298822016), '4': np.float64(0.2655270267694501)}\n",
            "lbdataDict {'123': np.float64(7.007007007007007e-05), '124': np.float64(0.0001981981981981982), '134': np.float64(7.007007007007007e-05), '234': np.float64(7.007007007007007e-05), '12': np.float64(0.0001981981981981982), '13': np.float64(7.007007007007007e-05), '14': np.float64(0.0001981981981981982), '23': np.float64(7.007007007007007e-05), '24': np.float64(0.0001981981981981982), '34': np.float64(7.007007007007007e-05), '1': np.float64(0.001033033033033033), '2': np.float64(0.0006986986986986987), '3': np.float64(0.0007067067067067067), '4': np.float64(0.001281281281281281)}\n",
            "ipfTopkDict {'123': np.float64(0.005105105105105105), '134': np.float64(0.005105105105105105), '234': np.float64(0.005105105105105105), '13': np.float64(0.005105105105105105), '23': np.float64(0.005105105105105105), '34': np.float64(0.005105105105105105), '3': np.float64(0.005105105105105105)}\n",
            "blTopkDict {'123': np.float64(7.007007007007007e-05), '134': np.float64(7.007007007007007e-05), '234': np.float64(7.007007007007007e-05), '13': np.float64(7.007007007007007e-05), '23': np.float64(7.007007007007007e-05), '34': np.float64(7.007007007007007e-05), '3': np.float64(7.007007007007007e-05), '124': np.float64(0.0001981981981981982), '12': np.float64(0.0001981981981981982), '14': np.float64(0.0001981981981981982), '24': np.float64(0.0001981981981981982), '1': np.float64(0.0001981981981981982), '2': np.float64(0.0001981981981981982), '4': np.float64(0.0001981981981981982)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['234', '123', '23', '124', '24', '12', '134', '13', '14', '34']\n",
            "approxTopk_ipf ['234', '123', '23', '134', '34', '13', '3', '124', '24', '12', '2', '14', '4']\n",
            "approxTopk_bl ['234', '123', '23', '134', '34', '13', '3', '124', '24', '12', '2', '14', '4', '1']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.00428928928928929), '124': np.float64(0.004385385385385385), '134': np.float64(0.005843767027463545), '234': np.float64(0.00413913917458147), '12': np.float64(0.004888888888888889), '13': np.float64(0.006702642334274683), '14': np.float64(0.006711711711711712), '23': np.float64(0.00428928928928929), '24': np.float64(0.004711711711711712), '34': np.float64(0.007072072072072072), '1': np.float64(0.2309484484391278), '2': np.float64(0.1531846846725346), '3': np.float64(0.2608553539828807), '4': np.float64(0.2587682664070649)}\n",
            "lbdataDict {'123': np.float64(0.0001371371371371371), '124': np.float64(0.0002492492492492492), '134': np.float64(0.0001531531531531532), '234': np.float64(9.109109109109109e-05), '12': np.float64(0.0003613613613613613), '13': np.float64(0.0002222222222222222), '14': np.float64(0.0004264264264264264), '23': np.float64(0.0001371371371371371), '24': np.float64(0.0002492492492492492), '34': np.float64(0.0001531531531531532), '1': np.float64(0.001546546546546547), '2': np.float64(0.001028028028028028), '3': np.float64(0.001057057057057057), '4': np.float64(0.001918918918918919)}\n",
            "ipfTopkDict {'234': np.float64(0.001774774774774775), '123': np.float64(0.002432432432432432), '23': np.float64(0.002432432432432432), '134': np.float64(0.003078078078078078), '34': np.float64(0.003078078078078078), '13': np.float64(0.005105105105105105), '3': np.float64(0.005105105105105105), '124': np.float64(0.005753753753753753), '24': np.float64(0.005753753753753753), '12': np.float64(0.007302302302302302), '2': np.float64(0.007302302302302302), '14': np.float64(0.009414414414414414), '4': np.float64(0.009414414414414414)}\n",
            "blTopkDict {'234': np.float64(9.109109109109109e-05), '123': np.float64(0.0001371371371371371), '23': np.float64(0.0001371371371371371), '134': np.float64(0.0001531531531531532), '34': np.float64(0.0001531531531531532), '13': np.float64(0.0002222222222222222), '3': np.float64(0.0002222222222222222), '124': np.float64(0.0002492492492492492), '24': np.float64(0.0002492492492492492), '12': np.float64(0.0003613613613613613), '2': np.float64(0.0003613613613613613), '14': np.float64(0.0004264264264264264), '4': np.float64(0.0004264264264264264), '1': np.float64(0.0005945945945945946)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['234', '123', '23', '134', '124', '24', '12', '34', '13', '14']\n",
            "approxTopk_ipf ['234', '123', '23', '134', '34', '124', '24', '13', '3', '12', '2', '14', '4']\n",
            "approxTopk_bl ['234', '123', '23', '134', '34', '124', '24', '13', '3', '12', '2', '14', '4', '1']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.003765752592873642), '124': np.float64(0.00409909925710578), '134': np.float64(0.004211211300297479), '234': np.float64(0.003384384384384384), '12': np.float64(0.004525650650650651), '13': np.float64(0.005823823823823824), '14': np.float64(0.006381381381381382), '23': np.float64(0.003819819819816042), '24': np.float64(0.00443556056056056), '34': np.float64(0.0059009009009009), '1': np.float64(0.2070004895991232), '2': np.float64(0.1450695695612195), '3': np.float64(0.2477812812647491), '4': np.float64(0.2438033032404573)}\n",
            "lbdataDict {'123': np.float64(0.0003443443443443443), '124': np.float64(0.0006546546546546547), '134': np.float64(0.0005175175175175176), '234': np.float64(0.0002602602602602602), '12': np.float64(0.0008548548548548548), '13': np.float64(0.0007197197197197198), '14': np.float64(0.001303303303303303), '23': np.float64(0.0003443443443443443), '24': np.float64(0.0006546546546546547), '34': np.float64(0.0005175175175175176), '1': np.float64(0.003075075075075075), '2': np.float64(0.002004004004004004), '3': np.float64(0.002396396396396396), '4': np.float64(0.00381981981981982)}\n",
            "ipfTopkDict {'234': np.float64(0.001995995995995996), '123': np.float64(0.00314014014014014), '23': np.float64(0.00314014014014014), '134': np.float64(0.003747747747747748), '34': np.float64(0.003747747747747748), '124': np.float64(0.004990990990990991), '24': np.float64(0.004990990990990991), '13': np.float64(0.006048048048048048), '3': np.float64(0.006048048048048048), '12': np.float64(0.007302302302302302), '2': np.float64(0.007302302302302302), '14': np.float64(0.009414414414414414), '4': np.float64(0.009414414414414414)}\n",
            "blTopkDict {'234': np.float64(0.0002602602602602602), '123': np.float64(0.0003443443443443443), '23': np.float64(0.0003443443443443443), '134': np.float64(0.0005175175175175176), '34': np.float64(0.0005175175175175176), '124': np.float64(0.0006546546546546547), '24': np.float64(0.0006546546546546547), '13': np.float64(0.0007197197197197198), '3': np.float64(0.0007197197197197198), '12': np.float64(0.0008548548548548548), '2': np.float64(0.0008548548548548548), '14': np.float64(0.001303303303303303), '4': np.float64(0.001303303303303303), '1': np.float64(0.001771771771771772)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['234', '123', '23', '124', '134', '24', '12', '34', '13', '14']\n",
            "approxTopk_ipf ['234', '123', '23', '24', '134', '34', '124', '12', '2', '13', '3', '14', '4']\n",
            "approxTopk_bl ['234', '123', '23', '124', '24', '134', '34', '12', '2', '13', '3', '14', '4', '1']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.002551551551551551), '124': np.float64(0.002921921921921922), '134': np.float64(0.003328328328328328), '234': np.float64(0.002627627627627628), '12': np.float64(0.00352052052052052), '13': np.float64(0.004783783783783784), '14': np.float64(0.005825825825825826), '23': np.float64(0.003048048192752339), '24': np.float64(0.003847847847847848), '34': np.float64(0.004924924924924925), '1': np.float64(0.1732407407407304), '2': np.float64(0.1376310998453328), '3': np.float64(0.2297552132009351), '4': np.float64(0.2251025470507299)}\n",
            "lbdataDict {'123': np.float64(0.0004554554554554555), '124': np.float64(0.0007767767767767768), '134': np.float64(0.0008548548548548548), '234': np.float64(0.0003023023023023023), '12': np.float64(0.001125125125125125), '13': np.float64(0.001252252252252252), '14': np.float64(0.002226226226226226), '23': np.float64(0.0004554554554554555), '24': np.float64(0.0007767767767767768), '34': np.float64(0.0008548548548548548), '1': np.float64(0.004084084084084084), '2': np.float64(0.002638638638638639), '3': np.float64(0.003083083083083083), '4': np.float64(0.005077077077077077)}\n",
            "ipfTopkDict {'234': np.float64(0.001159159159159159), '123': np.float64(0.001761761761761762), '23': np.float64(0.001761761761761762), '24': np.float64(0.003427427427427427), '134': np.float64(0.003501501501501502), '34': np.float64(0.003501501501501502), '124': np.float64(0.003690690690690691), '12': np.float64(0.004857857857857858), '2': np.float64(0.004857857857857858), '13': np.float64(0.005737737737737738), '3': np.float64(0.005737737737737738), '14': np.float64(0.009414414414414414), '4': np.float64(0.009414414414414414)}\n",
            "blTopkDict {'234': np.float64(0.0003023023023023023), '123': np.float64(0.0004554554554554555), '23': np.float64(0.0004554554554554555), '124': np.float64(0.0007767767767767768), '24': np.float64(0.0007767767767767768), '134': np.float64(0.0008548548548548548), '34': np.float64(0.0008548548548548548), '12': np.float64(0.001125125125125125), '2': np.float64(0.001125125125125125), '13': np.float64(0.001252252252252252), '3': np.float64(0.001252252252252252), '14': np.float64(0.002226226226226226), '4': np.float64(0.002226226226226226), '1': np.float64(0.003147147147147147)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['123', '234', '124', '23', '12', '134', '24', '13', '34', '14']\n",
            "approxTopk_ipf ['234', '123', '23', '124', '24', '134', '34', '12', '2', '13', '3', '14', '4']\n",
            "approxTopk_bl ['234', '123', '23', '124', '24', '134', '34', '12', '2', '13', '3', '14', '4', '1']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.001760760760760761), '124': np.float64(0.002128128140766577), '134': np.float64(0.003042042042042042), '234': np.float64(0.002339339363038492), '12': np.float64(0.00264064064064064), '13': np.float64(0.003923922944445684), '14': np.float64(0.005045045045045045), '23': np.float64(0.002951951951951952), '24': np.float64(0.003607721053946488), '34': np.float64(0.004908908908908909), '1': np.float64(0.1293808808808809), '2': np.float64(0.1266781667842252), '3': np.float64(0.2059273710172242), '4': np.float64(0.2035700080737334)}\n",
            "lbdataDict {'123': np.float64(0.0006286286286286286), '124': np.float64(0.000978978978978979), '134': np.float64(0.001211211211211211), '234': np.float64(0.0004054054054054054), '12': np.float64(0.001483483483483484), '13': np.float64(0.001870870870870871), '14': np.float64(0.002927927927927928), '23': np.float64(0.0006286286286286286), '24': np.float64(0.000978978978978979), '34': np.float64(0.001211211211211211), '1': np.float64(0.004585585585585586), '2': np.float64(0.002952952952952953), '3': np.float64(0.003725725725725726), '4': np.float64(0.005702702702702703)}\n",
            "ipfTopkDict {'234': np.float64(0.001384384384384384), '123': np.float64(0.002227227227227227), '23': np.float64(0.002227227227227227), '124': np.float64(0.003126126126126126), '24': np.float64(0.003126126126126126), '134': np.float64(0.003765765765765766), '34': np.float64(0.003765765765765766), '12': np.float64(0.004906906906906907), '2': np.float64(0.004906906906906907), '13': np.float64(0.006048048048048048), '3': np.float64(0.006048048048048048), '14': np.float64(0.009414414414414414), '4': np.float64(0.009414414414414414)}\n",
            "blTopkDict {'234': np.float64(0.0004054054054054054), '123': np.float64(0.0006286286286286286), '23': np.float64(0.0006286286286286286), '124': np.float64(0.000978978978978979), '24': np.float64(0.000978978978978979), '134': np.float64(0.001211211211211211), '34': np.float64(0.001211211211211211), '12': np.float64(0.001483483483483484), '2': np.float64(0.001483483483483484), '13': np.float64(0.001870870870870871), '3': np.float64(0.001870870870870871), '14': np.float64(0.002927927927927928), '4': np.float64(0.002927927927927928), '1': np.float64(0.004432432432432433)}\n",
            "actualTopk ['123', '234', '124', '134', '23', '12', '13', '24', '34', '14', '2', '3', '1', '4']\n",
            "approxTopk_lpds ['123', '234', '124', '23', '134', '12', '24', '13', '34', '14']\n",
            "approxTopk_ipf ['234', '134', '123', '23', '13', '124', '24', '34', '14', '12', '2', '3', '1', '4']\n",
            "approxTopk_bl ['234', '123', '23', '124', '134', '24', '12', '34', '2', '13', '3', '14', '4', '1']\n",
            "approxTopk_pyroplus ['34', '3']\n",
            "actualTopkDict {'123': 0.001016016016016016, '234': 0.0012242242242242241, '124': 0.0015405405405405405, '134': 0.0019139139139139139, '23': 0.001990990990990991, '12': 0.0024564564564564565, '13': 0.003051051051051051, '24': 0.003058058058058058, '34': 0.003792792792792793, '14': 0.004741741741741742, '2': 0.00487987987987988, '3': 0.006048048048048048, '1': 0.0074444444444444445, '4': 0.009414414414414414}\n",
            "ubdataDict {'123': np.float64(0.001541534811285785), '124': np.float64(0.00206006006006006), '134': np.float64(0.002614614614614615), '234': np.float64(0.002019019019019019), '12': np.float64(0.002615615615615616), '13': np.float64(0.003651651651651652), '14': np.float64(0.004741741741741742), '23': np.float64(0.002741741741741742), '24': np.float64(0.003551551551551551), '34': np.float64(0.004670670670670671), '1': np.float64(0.1174979186303709), '2': np.float64(0.1070635225953757), '3': np.float64(0.1763648648648259), '4': np.float64(0.1705319483874917)}\n",
            "lbdataDict {'123': np.float64(0.0006776776776776777), '124': np.float64(0.001061061061061061), '134': np.float64(0.001238238238238238), '234': np.float64(0.0005265265265265265), '12': np.float64(0.001616616616616617), '13': np.float64(0.001956956956956957), '14': np.float64(0.00301001001001001), '23': np.float64(0.0007947947947947948), '24': np.float64(0.001332332332332332), '34': np.float64(0.00164964964964965), '1': np.float64(0.005067067067067067), '2': np.float64(0.003299299299299299), '3': np.float64(0.004066066066066066), '4': np.float64(0.006326326326326326)}\n",
            "ipfTopkDict {'234': np.float64(0.001341341341341341), '134': np.float64(0.001748748748748749), '123': np.float64(0.002003003003003003), '23': np.float64(0.002223223223223223), '13': np.float64(0.002888888888888889), '124': np.float64(0.003155155155155155), '24': np.float64(0.003273273273273273), '34': np.float64(0.003774774774774775), '14': np.float64(0.005186186186186186), '12': np.float64(0.005224224224224224), '2': np.float64(0.005224224224224224), '3': np.float64(0.006048048048048048), '1': np.float64(0.007444444444444445), '4': np.float64(0.009414414414414414)}\n",
            "blTopkDict {'234': np.float64(0.0005265265265265265), '123': np.float64(0.0006776776776776777), '23': np.float64(0.0007947947947947948), '124': np.float64(0.001061061061061061), '134': np.float64(0.001238238238238238), '24': np.float64(0.001332332332332332), '12': np.float64(0.001616616616616617), '34': np.float64(0.00164964964964965), '2': np.float64(0.001943943943943944), '13': np.float64(0.001956956956956957), '3': np.float64(0.002438438438438439), '14': np.float64(0.00301001001001001), '4': np.float64(0.004074074074074074), '1': np.float64(0.004667667667667668)}\n",
            "Total AFDs: 14\n",
            "AFDs found by AppxLP: 10\n",
            "AFDs found by AppxIPF: 14\n",
            "AFDs found by TANE: 14\n",
            "AFDs found by Pyro++: 2\n",
            "prec_lpds [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "prec_ipf [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "rec_lpds [0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143]\n",
            "rec_ipf [1.0, 0.5, 0.9285714285714286, 0.9285714285714286, 0.9285714285714286, 0.9285714285714286, 1.0]\n"
          ]
        }
      ],
      "source": [
        "AFDStartPoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}