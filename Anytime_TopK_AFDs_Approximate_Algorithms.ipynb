{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_MhVcpyrBAw"
      },
      "source": [
        "**Upper Bound (QP) - 2D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szxuoO4zb72t"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_ub_qp_2D(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "    n = len(row_sum)\n",
        "    m = len(col_sum)\n",
        "    seen_array = np.array(seen)\n",
        "\n",
        "    # optimization variables\n",
        "    matrix = cp.Variable((n, m), nonneg=True)\n",
        "\n",
        "    # precomputed row and column totals of the seen matrix\n",
        "    seen_row_sums = seen_array.sum(axis=1)\n",
        "    seen_col_sums = seen_array.sum(axis=0)\n",
        "\n",
        "    # objective: minimize sum of squares of seen + matrix\n",
        "    objective = cp.Minimize(cp.sum_squares(seen_array + matrix))\n",
        "\n",
        "    # constraints for row and column sums\n",
        "    constraints = [\n",
        "        cp.sum(matrix, axis=1) + seen_row_sums <= row_sum,\n",
        "        cp.sum(matrix, axis=0) + seen_col_sums <= col_sum\n",
        "    ]\n",
        "\n",
        "    # define solver\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve(solver=cp.SCS)\n",
        "\n",
        "    if problem.status != cp.OPTIMAL:\n",
        "        raise ValueError(\"Optimization was not successful\")\n",
        "\n",
        "    g1ErrorUB = problem.value\n",
        "    row_square = np.dot(row_sum, row_sum)\n",
        "    conflict = (row_square - g1ErrorUB) / 2\n",
        "    return conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVt0GPXrHAg"
      },
      "source": [
        "**Lower Bound (Ignore Row Sum) - 2D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OANm_ZaAcENp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_lb_ignoreSum_2D(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "  row_sum_cur = []\n",
        "  row_max_index = []\n",
        "  remaining_row_sum = []\n",
        "\n",
        "  n = len(row_sum)\n",
        "  m = len(col_sum)\n",
        "\n",
        "  # Calculate conflict\n",
        "  xi_square = 0\n",
        "  for i in range(n):\n",
        "    row_sum_cur.append(0)\n",
        "    row_max_index.append(0)\n",
        "    remaining_row_sum.append(0)\n",
        "    for j in range(m):\n",
        "      row_sum_cur[i] += seen[i][j]\n",
        "      if seen[i][row_max_index[i]] < seen[i][j]:\n",
        "        row_max_index[i] = j\n",
        "      xi_square += seen[i][j]*seen[i][j]\n",
        "    remaining_row_sum[i] = row_sum[i] - row_sum_cur[i]\n",
        "    _max = seen[i][row_max_index[i]]\n",
        "    xi_square += remaining_row_sum[i] * remaining_row_sum[i] + 2 * _max * remaining_row_sum[i]\n",
        "  row_square = 0\n",
        "  for r_s in row_sum:\n",
        "    row_square += r_s*r_s\n",
        "  conflict = (row_square - xi_square)/2\n",
        "  return conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsSjPiE4bCoy"
      },
      "source": [
        "**Bounds (QP) - 3D**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xtsgvG0JZu7",
        "outputId": "693b25a4-e0e5-4ada-f2f7-e9b58766f98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (12.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADAFGsKGH88"
      },
      "outputs": [],
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_bounds_qp_3D(\n",
        "    seen: List[List[List[int]]],\n",
        "    row_sum_1: List[int],\n",
        "    row_sum_2: List[int],\n",
        "    col_sum: List[int],\n",
        "    lb_calc: bool\n",
        ") -> int:\n",
        "\n",
        "    n1 = len(row_sum_1)\n",
        "    n2 = len(row_sum_2)\n",
        "    m = len(col_sum)\n",
        "\n",
        "    seen_np = np.array(seen)\n",
        "\n",
        "    model = gp.Model()\n",
        "    model.setParam(\"OutputFlag\", 0)\n",
        "\n",
        "    # non-negative decision variables\n",
        "    matrix_vars = model.addVars(n1, n2, m, lb=0, name=\"matrix\")\n",
        "\n",
        "    # objective function: sum over (seen + x)[i,j,k1] * (seen + x)[i,j,k2] for k1 < k2\n",
        "    objective_terms = []\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            for k1 in range(m - 1):\n",
        "                for k2 in range(k1 + 1, m):\n",
        "                    t1 = seen_np[i, j, k1] + matrix_vars[i, j, k1]\n",
        "                    t2 = seen_np[i, j, k2] + matrix_vars[i, j, k2]\n",
        "                    objective_terms.append(t1 * t2)\n",
        "\n",
        "    model.setObjective(gp.quicksum(objective_terms), gp.GRB.MINIMIZE if lb_calc else gp.GRB.MAXIMIZE)\n",
        "\n",
        "    # row sum constraints\n",
        "    for i in range(n1):\n",
        "        rhs = row_sum_1[i] - seen_np[i, :, :].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for j in range(n2) for k in range(m)) <= rhs,\n",
        "            name=f\"RowSum1_{i}\"\n",
        "        )\n",
        "\n",
        "    for j in range(n2):\n",
        "        rhs = row_sum_2[j] - seen_np[:, j, :].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for i in range(n1) for k in range(m)) <= rhs,\n",
        "            name=f\"RowSum2_{j}\"\n",
        "        )\n",
        "\n",
        "    # col sum constraints\n",
        "    for k in range(m):\n",
        "        rhs = col_sum[k] - seen_np[:, :, k].sum()\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix_vars[i, j, k] for i in range(n1) for j in range(n2)) <= rhs,\n",
        "            name=f\"ColSum_{k}\"\n",
        "        )\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == gp.GRB.OPTIMAL:\n",
        "        return model.objVal\n",
        "    else:\n",
        "        raise ValueError(\"No optimal solution found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MuP7YYXbMPY"
      },
      "source": [
        "**Bounds (QP) - 4D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IZqungbNZsV"
      },
      "outputs": [],
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def g1_bounds_qp_4D(\n",
        "    seen: List[List[List[List[int]]]],\n",
        "    row_sum_1: List[int],\n",
        "    row_sum_2: List[int],\n",
        "    row_sum_3: List[int],\n",
        "    col_sum: List[int],\n",
        "    lb_calc: bool\n",
        ") -> int:\n",
        "\n",
        "    n1 = len(row_sum_1)\n",
        "    n2 = len(row_sum_2)\n",
        "    n3 = len(row_sum_3)\n",
        "    m = len(col_sum)\n",
        "\n",
        "    seen_np = np.array(seen)\n",
        "\n",
        "    model = gp.Model()\n",
        "    model.setParam(\"OutputFlag\", 0)\n",
        "\n",
        "    # decision variables: non-negative\n",
        "    matrix_vars = model.addVars(n1, n2, n3, m, lb=0, name=\"matrix\")\n",
        "\n",
        "    # objective: sum over (seen + matrix)[i,j,k,l1] * (seen + matrix)[i,j,k,l2] for l1 < l2\n",
        "    objective_terms = []\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            for k in range(n3):\n",
        "                for l1 in range(m - 1):\n",
        "                    for l2 in range(l1 + 1, m):\n",
        "                        term1 = seen_np[i, j, k, l1] + matrix_vars[i, j, k, l1]\n",
        "                        term2 = seen_np[i, j, k, l2] + matrix_vars[i, j, k, l2]\n",
        "                        objective_terms.append(term1 * term2)\n",
        "\n",
        "    if lb_calc:\n",
        "        model.setObjective(gp.quicksum(objective_terms), gp.GRB.MINIMIZE)\n",
        "    else:\n",
        "        model.setObjective(gp.quicksum(objective_terms), gp.GRB.MAXIMIZE)\n",
        "\n",
        "    # Constraints\n",
        "\n",
        "    # 1D row sums\n",
        "    for i in range(n1):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for j in range(n2) for k in range(n3) for l in range(m))\n",
        "        rhs = row_sum_1[i] - seen_np[i, :, :, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum1_{i}\")\n",
        "\n",
        "    # 2D row sums\n",
        "    for j in range(n2):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for k in range(n3) for l in range(m))\n",
        "        rhs = row_sum_2[j] - seen_np[:, j, :, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum2_{j}\")\n",
        "\n",
        "    # 3D row sums\n",
        "    for k in range(n3):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for j in range(n2) for l in range(m))\n",
        "        rhs = row_sum_3[k] - seen_np[:, :, k, :].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"RowSum3_{k}\")\n",
        "\n",
        "    # Column sums\n",
        "    for l in range(m):\n",
        "        lhs = gp.quicksum(matrix_vars[i, j, k, l] for i in range(n1) for j in range(n2) for k in range(n3))\n",
        "        rhs = col_sum[l] - seen_np[:, :, :, l].sum()\n",
        "        model.addConstr(lhs <= rhs, name=f\"ColSum_{l}\")\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == gp.GRB.OPTIMAL:\n",
        "        return model.objVal\n",
        "    else:\n",
        "        raise ValueError(\"No optimal solution found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mjiMR9dkVHz"
      },
      "source": [
        "**Approx Top-K Ranking (Sampling/Mean)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZLEkZhlAgx_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Approach 1 : Using mean to find top-k\n",
        "def cal_mean(attributes, ubDict, lbDict, e):\n",
        "\n",
        "  meanTopk = []\n",
        "  emax_attr = []\n",
        "\n",
        "  for attribute in attributes:\n",
        "    lowerB = lbDict[attribute]\n",
        "    upperB = ubDict[attribute]\n",
        "\n",
        "    if upperB <= e:\n",
        "        emax_attr.append(attribute)\n",
        "\n",
        "  for attr in emax_attr:\n",
        "    lb = lbDict[attr]\n",
        "    ub = ubDict[attr]\n",
        "    meanVal = (lb+ub)/2\n",
        "    meanTopk.append((attr, meanVal))\n",
        "\n",
        "  meanTopk = sorted(meanTopk, key=lambda x: x[1])\n",
        "\n",
        "  meanTopk = [attr[0] for attr in meanTopk]\n",
        "\n",
        "  return meanTopk\n",
        "\n",
        "# Approach 2 : Using sampling to find top-k\n",
        "def cal_sample_prob(attributes, ubDict, lbDict, e):\n",
        "\n",
        "  emax_attr = []\n",
        "  order_counts = defaultdict(int)\n",
        "\n",
        "  for attribute in attributes:\n",
        "    lowerB = lbDict[attribute]\n",
        "    upperB = ubDict[attribute]\n",
        "\n",
        "    if upperB <= e:\n",
        "      emax_attr.append(attribute)\n",
        "\n",
        "  for _ in range(1000):\n",
        "    samples = {}\n",
        "    for attr in emax_attr:\n",
        "      lower_bound = lbDict[attr]\n",
        "      upper_bound = ubDict[attr]\n",
        "      samples[str(attr)] = random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "    sorted_order = tuple(sorted(samples, key=samples.get))\n",
        "    order_counts[sorted_order] += 1\n",
        "\n",
        "  most_frequent_order = max(order_counts, key=order_counts.get)\n",
        "\n",
        "  return most_frequent_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMTcJ1qDsO_X"
      },
      "source": [
        "**Actual Top-K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J05xQ4sSN0I0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "\n",
        "def calculateActualTopk(file_name, e):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  actualConflictDict_4D = {}\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "    g1Error_4D = 0\n",
        "\n",
        "    for matrix_3d in matrix_3lhs:\n",
        "      for matrix_2d in matrix_3d:\n",
        "          for row in matrix_2d:\n",
        "              pairs = itertools.combinations(row, 2)\n",
        "              for pair in pairs:\n",
        "                  g1Error_4D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_4D[str(A_name)] = g1Error_4D/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  actualConflictDict_3D = {}\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "    g1Error_3D = 0\n",
        "\n",
        "    for matrix_2d in matrix_2lhs:\n",
        "      for row in matrix_2d:\n",
        "          pairs = itertools.combinations(row, 2)\n",
        "          for pair in pairs:\n",
        "              g1Error_3D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_3D[str(A_name)] = g1Error_3D/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  actualConflictDict_2D = {}\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "    g1Error_2D = 0\n",
        "    for row in matrix:\n",
        "      pairs = itertools.combinations(row, 2)\n",
        "      for pair in pairs:\n",
        "        g1Error_2D += pair[0] * pair[1]\n",
        "\n",
        "    actualConflictDict_2D[A_name] = g1Error_2D/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  mergedConflictDict = {**actualConflictDict_2D, **actualConflictDict_3D, **actualConflictDict_4D}\n",
        "  unsortedConflictDict = {k: v for k, v in mergedConflictDict.items() if v <= e}\n",
        "  sortedConflictDict = {k: v for k, v in sorted(mergedConflictDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "  actualTopk = sorted(sortedConflictDict, key=lambda k: sortedConflictDict[k])\n",
        "\n",
        "  return sortedConflictDict, unsortedConflictDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYAvL8msS4N"
      },
      "source": [
        "**LPDS - Uninformed Prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5N-mBOcVkzq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def removeSingleMarginals(calc_matrix, row_sum, col_sum):\n",
        "\n",
        "  row_sum_copy = row_sum[:]\n",
        "  col_sum_copy = col_sum[:]\n",
        "  matrix_copy = [row[:] for row in calc_matrix]\n",
        "\n",
        "  rows_to_remove = [i for i, rsum in enumerate(row_sum_copy) if rsum == 1]\n",
        "\n",
        "  for i in sorted(rows_to_remove, reverse=True):\n",
        "        row = matrix_copy[i]\n",
        "        for j, val in enumerate(row):\n",
        "            col_sum_copy[j] -= val\n",
        "\n",
        "        del matrix_copy[i]\n",
        "        del row_sum_copy[i]\n",
        "\n",
        "  return matrix_copy, row_sum_copy, col_sum_copy\n",
        "\n",
        "def calculateBoundsLPDS(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainUBDict = {}\n",
        "  mainLBDict = {}\n",
        "\n",
        "  boundsTime2D = {}\n",
        "  boundsTime3D = {}\n",
        "  boundsTime4D = {}\n",
        "\n",
        "  ub2TimeDict = {}\n",
        "  ub3TimeDict = {}\n",
        "  ub4TimeDict = {}\n",
        "  lb2TimeDict = {}\n",
        "  lb3TimeDict = {}\n",
        "  lb4TimeDict = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    op1 = []\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainUBDict:\n",
        "            mainUBDict[index] = {}\n",
        "\n",
        "          if index not in mainLBDict:\n",
        "            mainLBDict[index] = {}\n",
        "\n",
        "          startTime4D = time.time()\n",
        "\n",
        "          startUBTime4D = time.time()\n",
        "          ubValue = g1_bounds_qp_4D(matrix_3lhs, rowSum_a1, rowSum_a2, rowSum_a3, colSum, False)\n",
        "          stopUBTime4D = time.time()\n",
        "\n",
        "          startLBTime4D = time.time()\n",
        "          lbValue = g1_bounds_qp_4D(matrix_3lhs, rowSum_a1, rowSum_a2, rowSum_a3, colSum, True)\n",
        "          stopLBTime4D = time.time()\n",
        "\n",
        "          stopTime4D = time.time()\n",
        "\n",
        "          boundsTime4D[index] = stopTime4D - startTime4D\n",
        "\n",
        "          ub4TimeDict[index] = stopUBTime4D - startUBTime4D\n",
        "          lb4TimeDict[index] = stopLBTime4D - startLBTime4D\n",
        "\n",
        "          ubValue = ubValue/norm\n",
        "          lbValue = lbValue/norm\n",
        "\n",
        "          if not op1:\n",
        "            op1.append([index+1, ubValue, lbValue])\n",
        "            u1 = ubValue\n",
        "            l2 = lbValue\n",
        "          else:\n",
        "            op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "            u1 = min(ubValue, op1[-1][1])\n",
        "            l2 = max(lbValue, op1[-1][2])\n",
        "\n",
        "          mainUBDict[index][A_name] = u1\n",
        "          mainLBDict[index][A_name] = l2\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    op1 = []\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          startTime3D = time.time()\n",
        "\n",
        "          startUBTime3D = time.time()\n",
        "          ubValue = g1_bounds_qp_3D(matrix_2lhs, rowSum_a1, rowSum_a2, colSum, False)\n",
        "          stopUBTime3D = time.time()\n",
        "\n",
        "          startLBTime3D = time.time()\n",
        "          lbValue = g1_bounds_qp_3D(matrix_2lhs, rowSum_a1, rowSum_a2, colSum, True)\n",
        "          stopLBTime3D = time.time()\n",
        "\n",
        "          stopTime3D = time.time()\n",
        "\n",
        "          boundsTime3D[index] = stopTime3D - startTime3D\n",
        "\n",
        "          ub3TimeDict[index] = stopUBTime3D - startUBTime3D\n",
        "          lb3TimeDict[index] = stopLBTime3D - startLBTime3D\n",
        "\n",
        "          ubValue = ubValue/norm\n",
        "          lbValue = lbValue/norm\n",
        "\n",
        "          if not op1:\n",
        "            op1.append([index+1, ubValue, lbValue])\n",
        "            u1 = ubValue\n",
        "            l2 = lbValue\n",
        "          else:\n",
        "            op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "            u1 = min(ubValue, op1[-1][1])\n",
        "            l2 = max(lbValue, op1[-1][2])\n",
        "\n",
        "          mainUBDict[index][A_name] = u1\n",
        "          mainLBDict[index][A_name] = l2\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    op1 = []\n",
        "    calc_matrix = []\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        mat, rowS, colS = removeSingleMarginals(calc_matrix, row_sum, col_sum)\n",
        "\n",
        "        startTime2D = time.time()\n",
        "\n",
        "        startUBTime2D = time.time()\n",
        "        ubValue = g1_ub_qp_2D(mat, rowS, colS)\n",
        "        stopUBTime2D = time.time()\n",
        "\n",
        "        startLBTime2D = time.time()\n",
        "        lbValue = g1_lb_ignoreSum_2D(mat, rowS, colS)\n",
        "        stopLBTime2D = time.time()\n",
        "\n",
        "        stopTime2D = time.time()\n",
        "\n",
        "        ub2TimeDict[index] = stopUBTime2D - startUBTime2D\n",
        "        lb2TimeDict[index] = stopLBTime2D - startLBTime2D\n",
        "\n",
        "        boundsTime2D[index] = stopTime2D - startTime2D\n",
        "\n",
        "        ubValue = ubValue/norm\n",
        "        lbValue = lbValue/norm\n",
        "\n",
        "        if not op1:\n",
        "          op1.append([index+1, ubValue, lbValue])\n",
        "          u1 = ubValue\n",
        "          l1 = lbValue\n",
        "\n",
        "        else:\n",
        "          op1.append([index+1, min(ubValue, op1[-1][1]), max(lbValue, op1[-1][2])])\n",
        "          u1 = min(ubValue, op1[-1][1])\n",
        "          l1 = max(lbValue, op1[-1][2])\n",
        "\n",
        "        mainUBDict[index][A_name] = u1\n",
        "        mainLBDict[index][A_name] = l1\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  indexTotalTimeDict = {key: boundsTime2D.get(key, 0) + boundsTime3D.get(key, 0) + boundsTime4D.get(key, 0)\n",
        "               for key in set(boundsTime2D) | set(boundsTime3D) | set(boundsTime4D)}\n",
        "\n",
        "  indexTotalUBTimeDict = {key: ub2TimeDict.get(key, 0) + ub3TimeDict.get(key, 0) + ub4TimeDict.get(key, 0)\n",
        "               for key in set(ub2TimeDict) | set(ub3TimeDict) | set(ub4TimeDict)}\n",
        "\n",
        "  indexTotalLBTimeDict = {key: lb2TimeDict.get(key, 0) + lb3TimeDict.get(key, 0) + lb4TimeDict.get(key, 0)\n",
        "               for key in set(lb2TimeDict) | set(lb3TimeDict) | set(lb4TimeDict)}\n",
        "\n",
        "  df1 = pd.DataFrame(list(boundsTime2D.items()), columns=['Index', 'Time'])\n",
        "  df2 = pd.DataFrame(list(boundsTime3D.items()), columns=['Index', 'Time'])\n",
        "  df3 = pd.DataFrame(list(boundsTime4D.items()), columns=['Index', 'Time'])\n",
        "  df4 = pd.DataFrame(list(indexTotalTimeDict.items()), columns=['Index', 'Time'])\n",
        "  df5 = pd.DataFrame(list(indexTotalUBTimeDict.items()), columns=['Index', 'Time'])\n",
        "  df6 = pd.DataFrame(list(indexTotalLBTimeDict.items()), columns=['Index', 'Time'])\n",
        "\n",
        "  df1.to_excel('2DTime.xlsx', index=False)\n",
        "  df2.to_excel('3DTime.xlsx', index=False)\n",
        "  df3.to_excel('4DTime.xlsx', index=False)\n",
        "  df4.to_excel('TotalTimeLP.xlsx', index=False)\n",
        "  df5.to_excel('AblationStudyUB.xlsx', index=False)\n",
        "  df6.to_excel('AblationStudyLB.xlsx', index=False)\n",
        "\n",
        "  return mainUBDict, mainLBDict, indexTotalTimeDict, boundsTime2D, boundsTime3D, boundsTime4D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIJkYq-a9hFn"
      },
      "source": [
        "**IPF - Informed Prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYxn_M_i9ox2",
        "outputId": "1c7a7bc1-3603-4cea-c862-4eb750f98149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipfn in /usr/local/lib/python3.11/dist-packages (1.4.4)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from ipfn) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipfn) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->ipfn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.22.0->ipfn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipfn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipfn import ipfn\n",
        "\n",
        "def calError_2D(matrix):\n",
        "  error = 0\n",
        "  for row in matrix:\n",
        "    pairs = itertools.combinations(row, 2)\n",
        "    for pair in pairs:\n",
        "      error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calError_3D(matrix):\n",
        "  error = 0\n",
        "  for matrix_2d in matrix:\n",
        "    for row in matrix_2d:\n",
        "      pairs = itertools.combinations(row, 2)\n",
        "      for pair in pairs:\n",
        "          error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calError_4D(matrix):\n",
        "  error = 0\n",
        "  for matrix_3d in matrix:\n",
        "    for matrix_2d in matrix_3d:\n",
        "        for row in matrix_2d:\n",
        "            pairs = itertools.combinations(row, 2)\n",
        "            for pair in pairs:\n",
        "                error += pair[0] * pair[1]\n",
        "  return error\n",
        "\n",
        "def calculateIPF(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainIPFDict = {}\n",
        "  boundsTimeIPF2D = {}\n",
        "  boundsTimeIPF3D = {}\n",
        "  boundsTimeIPF4D = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    dimensions_4D = [[0], [1], [2], [3]]\n",
        "    constraints_4D = [rowSum_a1, rowSum_a2, rowSum_a3, colSum]\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainIPFDict:\n",
        "            mainIPFDict[index] = {}\n",
        "\n",
        "          matrix_3lhs_np = np.array(matrix_3lhs)\n",
        "          startTimeIPF4D = time.time()\n",
        "          ipfMatrix_4D = ipfn.ipfn(matrix_3lhs_np, constraints_4D, dimensions_4D)\n",
        "          matrix_3lhs_ipf = np.round(ipfMatrix_4D.iteration()).astype(int)\n",
        "          error = calError_4D(matrix_3lhs_ipf)\n",
        "          stopTimeIPF4D = time.time()\n",
        "\n",
        "          boundsTimeIPF4D[index] = stopTimeIPF4D - startTimeIPF4D\n",
        "          mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    dimensions_3D = [[0], [1], [2]]\n",
        "    constraints_3D = [rowSum_a1, rowSum_a2, colSum]\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          matrix_2lhs_np = np.array(matrix_2lhs)\n",
        "          startTimeIPF3D = time.time()\n",
        "          ipfMatrix_3D = ipfn.ipfn(matrix_2lhs_np, constraints_3D, dimensions_3D)\n",
        "          matrix_2lhs_ipf = np.round(ipfMatrix_3D.iteration()).astype(int)\n",
        "          error = calError_3D(matrix_2lhs_ipf)\n",
        "          stopTimeIPF3D = time.time()\n",
        "\n",
        "          boundsTimeIPF3D[index] = stopTimeIPF3D - startTimeIPF3D\n",
        "          mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    calc_matrix = []\n",
        "    dimensions_2D = [[0], [1]]\n",
        "    constraints_2D = [row_sum, col_sum]\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        calc_matrix_np = np.array(calc_matrix)\n",
        "        startTimeIPF2D = time.time()\n",
        "        ipfMatrix_2D = ipfn.ipfn(calc_matrix_np, constraints_2D, dimensions_2D)\n",
        "        calc_matrix_ipf = np.round(ipfMatrix_2D.iteration()).astype(int)\n",
        "        error = calError_2D(calc_matrix_ipf)\n",
        "        stopTimeIPF2D = time.time()\n",
        "\n",
        "        boundsTimeIPF2D[index] = stopTimeIPF2D - startTimeIPF2D\n",
        "        mainIPFDict[index][str(A_name)] = error/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  indexTotalTimeDictIPF = {key: boundsTimeIPF2D.get(key, 0) + boundsTimeIPF3D.get(key, 0) + boundsTimeIPF4D.get(key, 0)\n",
        "               for key in set(boundsTimeIPF2D) | set(boundsTimeIPF3D) | set(boundsTimeIPF4D)}\n",
        "\n",
        "  return mainIPFDict, indexTotalTimeDictIPF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce6SldH9awn"
      },
      "source": [
        "**TANE/Greedy - Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMIRD2EI9gQR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculateBaseline(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "\n",
        "  numOfRows = len(mainDF)\n",
        "  norm = numOfRows*(numOfRows-1)\n",
        "\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  mainBLDict = {}\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  attribute_triplets = list(combinations(mainDF.columns[:-1], 3))\n",
        "\n",
        "  for attr1, attr2, attr3 in attribute_triplets:\n",
        "\n",
        "    A_name = attr1+attr2+attr3\n",
        "\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    attr3_values = mainDF[attr3].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_3lhs = [[[[0 for _ in target_values] for _ in attr3_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    attr3_indices_map = {value: idx for idx, value in enumerate(attr3_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    rowSum_a3 = [0]*len(attr3_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      a3_index = attr3_indices_map[row[attr3]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      rowSum_a3[a3_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      val3 = row[attr3]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and val3 in attr3_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        depth_index = attr3_indices_map[val3]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_3lhs[row_index][col_index][depth_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          if index not in mainBLDict:\n",
        "            mainBLDict[index] = {}\n",
        "\n",
        "          g1ErrorBL_4D = 0\n",
        "\n",
        "          for matrix_3d in matrix_3lhs:\n",
        "            for matrix_2d in matrix_3d:\n",
        "                for row in matrix_2d:\n",
        "                    pairs = itertools.combinations(row, 2)\n",
        "                    for pair in pairs:\n",
        "                        g1ErrorBL_4D += pair[0] * pair[1]\n",
        "\n",
        "          mainBLDict[index][str(A_name)] = g1ErrorBL_4D/norm\n",
        "\n",
        "  # THREE-LHS AFDs\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  attribute_pairs = list(combinations(mainDF.columns[:-1], 2))\n",
        "\n",
        "  for attr1, attr2 in attribute_pairs:\n",
        "\n",
        "    A_name = attr1+attr2\n",
        "    attr1_values = mainDF[attr1].unique().tolist()\n",
        "    attr2_values = mainDF[attr2].unique().tolist()\n",
        "    target_values = mainDF['y'].unique().tolist()\n",
        "\n",
        "    matrix_2lhs = [[[0 for _ in target_values] for _ in attr2_values] for _ in attr1_values]\n",
        "\n",
        "    attr1_indices_map = {value: idx for idx, value in enumerate(attr1_values)}\n",
        "    attr2_indices_map = {value: idx for idx, value in enumerate(attr2_values)}\n",
        "    target_indices_map = {value: idx for idx, value in enumerate(target_values)}\n",
        "\n",
        "    rowSum_a1 = [0]*len(attr1_indices_map)\n",
        "    rowSum_a2 = [0]*len(attr2_indices_map)\n",
        "    colSum = [0]*len(target_indices_map)\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "      a1_index = attr1_indices_map[row[attr1]]\n",
        "      a2_index = attr2_indices_map[row[attr2]]\n",
        "      z_index = target_indices_map[row['y']]\n",
        "\n",
        "      rowSum_a1[a1_index] += 1\n",
        "      rowSum_a2[a2_index] += 1\n",
        "      colSum[z_index] += 1\n",
        "\n",
        "    for index, row in mainDF.iterrows():\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      val1 = row[attr1]\n",
        "      val2 = row[attr2]\n",
        "      target_val = row['y']\n",
        "\n",
        "      if val1 in attr1_indices_map and val2 in attr2_indices_map and target_val in target_indices_map:\n",
        "\n",
        "        row_index = attr1_indices_map[val1]\n",
        "        col_index = attr2_indices_map[val2]\n",
        "        target_index = target_indices_map[target_val]\n",
        "\n",
        "        matrix_2lhs[row_index][col_index][target_index] += 1\n",
        "\n",
        "        if index % dataInterval == 0 or index == len(mainDF)-1:\n",
        "\n",
        "          g1ErrorBL_3D = 0\n",
        "\n",
        "          for matrix_2d in matrix_2lhs:\n",
        "            for row in matrix_2d:\n",
        "                pairs = itertools.combinations(row, 2)\n",
        "                for pair in pairs:\n",
        "                    g1ErrorBL_3D += pair[0] * pair[1]\n",
        "\n",
        "          mainBLDict[index][str(A_name)] = g1ErrorBL_3D/norm\n",
        "\n",
        "  # TWO-LHS AFDs\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        g1ErrorBL_2D = 0\n",
        "        for row in matrix:\n",
        "          pairs = itertools.combinations(row, 2)\n",
        "          for pair in pairs:\n",
        "            g1ErrorBL_2D += pair[0] * pair[1]\n",
        "\n",
        "        mainBLDict[index][str(A_name)] = g1ErrorBL_2D/norm\n",
        "\n",
        "  # ONE-LHS AFDs\n",
        "\n",
        "  return mainBLDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvIriRGPdFC1"
      },
      "source": [
        "**Pyro++ - SOTA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7amd1eUF38e1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "# converting log files generated by Pyro and pandas dataframe to extract AFDs\n",
        "def log_to_dataframe(log_file):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  with open(log_file, 'r') as log:\n",
        "    for line in log:\n",
        "      if \"LHS:\" in line and \"RHS:\" in line and \"Error:\" in line:\n",
        "        try:\n",
        "          lhs = line.split('LHS:')[1].split('RHS:')[0].strip().strip('[]').replace(']', '').replace(',', '').strip()\n",
        "          rhs = line.split('RHS:')[1].split('Error:')[0].strip().strip('[]').replace(']', '').replace(',', '').strip()\n",
        "          error = float(line.split('Error:')[1].strip())\n",
        "\n",
        "          data.append([lhs, rhs, error])\n",
        "        except IndexError as e:\n",
        "          print(f\"Skipping line due to error: {e}\")\n",
        "          continue\n",
        "\n",
        "  sorted_data = sorted(data, key=lambda x: x[2])\n",
        "  df = pd.DataFrame(sorted_data, columns=['LHS', 'RHS', 'Error'])\n",
        "\n",
        "  return df\n",
        "\n",
        "# adapting Pyro++ by generating supersets of minimal AFDs by Pyro\n",
        "def generate_supersets(sets_str):\n",
        "\n",
        "  supersets = []\n",
        "  sets = [set(map(int, s)) for s in sets_str]\n",
        "  unique_elements = set().union(*sets)\n",
        "\n",
        "  for r in range(1, 4):\n",
        "      for combo in combinations(unique_elements, r):\n",
        "          superset = set(combo)\n",
        "          if any(s.issubset(superset) for s in sets):\n",
        "              supersets.append(superset)\n",
        "\n",
        "  supersets = sorted(supersets, key=lambda x: (len(x), sorted(x)))\n",
        "\n",
        "  supersets_str = [''.join(sorted(map(str, superset))) for superset in supersets]\n",
        "\n",
        "  return supersets_str\n",
        "\n",
        "# score calculation for pyro++\n",
        "def calculatePyroPatkAndNDCG(filename, actualTopk, actualG1Dict, actualTopk_rs, k):\n",
        "\n",
        "  df_og = pd.read_csv(filename, header = 0)\n",
        "  pdbxCol = df_og.columns.tolist()\n",
        "  numCol = df_og.shape[1]\n",
        "\n",
        "  pyroplusTopk = []\n",
        "  patkGraph_pyro = []\n",
        "  ndcgatkGraph_pyro = []\n",
        "  folder_path = './' + 'logFiles'\n",
        "\n",
        "  for fn in sorted(os.listdir(folder_path)):\n",
        "\n",
        "    # note: only pass files upto the percent of data read (60% for our experiments), pass in correct order\n",
        "\n",
        "    log_file_path = os.path.join(folder_path, fn)\n",
        "    df_pyro = log_to_dataframe(log_file_path)\n",
        "    pyroTopK = []\n",
        "\n",
        "    for index, row in df_pyro.iterrows():\n",
        "\n",
        "      col1 = row['LHS']\n",
        "      col2 = row['RHS']\n",
        "\n",
        "      if col2[6:] == str(numCol):\n",
        "        col = re.sub(r'[^0-9]', '', col1)\n",
        "        pyroTopK.append(col)\n",
        "\n",
        "    pyroplusDictSorted = {}\n",
        "    pyroplusDictUnsorted = {}\n",
        "\n",
        "    if pyroTopK:\n",
        "\n",
        "      pyroplusList = generate_supersets(pyroTopK)\n",
        "      pyroplusDictUnsorted = {ky: v for ky, v in actualG1Dict.items() if ky in pyroplusList}\n",
        "      pyroplusDictSorted = dict(sorted(pyroplusDictUnsorted.items(), key=lambda item: item[1]))\n",
        "      pyroplusTopk = sorted(pyroplusDictSorted, key=lambda ky: pyroplusDictSorted[ky])\n",
        "\n",
        "      pyroTopk_rs = adjustRelevanceScore(pyroplusTopk, actualTopk[:k], actualTopk_rs[:k])\n",
        "\n",
        "      patkGraph_pyro.append(precision_at_k(actualTopk, pyroplusTopk, k=k))\n",
        "      ndcgatkGraph_pyro.append(ndcg_at_k(pyroTopk_rs, actualTopk_rs, k=k))\n",
        "\n",
        "    else:\n",
        "      patkGraph_pyro.append(0.0)\n",
        "      ndcgatkGraph_pyro.append(0.0)\n",
        "\n",
        "  return patkGraph_pyro, ndcgatkGraph_pyro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nv40NLkrwdb"
      },
      "source": [
        "**Score Calculation - P@K/NDCG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHoXOPNIa2rp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Precision at k score\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "\n",
        "  relevantItems = sum([1 for item in y_pred[:k] if item in y_true[:k]])\n",
        "\n",
        "  return relevantItems/k\n",
        "\n",
        "# ndcg - true relevance score\n",
        "def calcRelevanceScore(topkDict, fixedIndexDict, k):\n",
        "\n",
        "  true_rs = [0] * len(fixedIndexDict)\n",
        "\n",
        "  if topkDict:\n",
        "    maxValNorm = topkDict[max(topkDict, key=topkDict.get)]\n",
        "    for key, value in topkDict.items():\n",
        "      if maxValNorm != 0:\n",
        "        normVal_RS = 1-(value/maxValNorm)\n",
        "      else:\n",
        "        normVal_RS = 0\n",
        "\n",
        "      if key in fixedIndexDict:\n",
        "        true_rs[fixedIndexDict[key]] = normVal_RS\n",
        "\n",
        "  return true_rs\n",
        "\n",
        "# ndcg - predicted relevance score\n",
        "def adjustRelevanceScore(predTopk, trueTopk, true_rs):\n",
        "\n",
        "  relMap = dict(zip(trueTopk, true_rs))\n",
        "  predTopk = predTopk[:len(trueTopk)]\n",
        "\n",
        "  if predTopk:\n",
        "    pred_rs = [relMap.get(item, 0) for item in predTopk]\n",
        "    pred_rs += [0] * (len(trueTopk) - len(predTopk))\n",
        "  else:\n",
        "    pred_rs = [0] * len(trueTopk)\n",
        "\n",
        "  return pred_rs\n",
        "\n",
        "# dcg score\n",
        "def dcg_at_k(relevance_scores, k):\n",
        "\n",
        "  dcg = 0.0\n",
        "  for i in range(min(k, len(relevance_scores))):\n",
        "      dcg += (2**relevance_scores[i] - 1) / np.log2(i + 2)\n",
        "\n",
        "  return dcg\n",
        "\n",
        "# ncdg score\n",
        "def ndcg_at_k(pred_rel, true_rel, k):\n",
        "\n",
        "  dcg_max = dcg_at_k(pred_rel, k)\n",
        "  ideal_relevance_scores = sorted(true_rel, reverse=True)\n",
        "  idcg = dcg_at_k(ideal_relevance_scores, k)\n",
        "\n",
        "  return dcg_max/idcg if idcg > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6OJBRSeJz9"
      },
      "source": [
        "**Generate Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1xSiwPGsVsl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Main Method: makes calls to all algorithms for bound calculation\n",
        "def calculatePatKandNDCG(file_name, dataInterval, rows, kList, stopAtIndex, fn, sortedStr, e):\n",
        "\n",
        "  # actual top-k\n",
        "  actualTopkDict, actualDict = calculateActualTopk(file_name, e)\n",
        "  actualTopk = sorted(actualTopkDict, key=lambda ky: actualTopkDict[ky])\n",
        "  fixedIndexAFDDict = {key: idx for idx, key in enumerate(actualTopkDict.keys())}\n",
        "\n",
        "  # time vs k graphs\n",
        "  lpdsTimeVsKDict = {}\n",
        "  ipfTimeVsKDict = {}\n",
        "  blTimeVsKDict = {}\n",
        "\n",
        "  for k in kList:\n",
        "\n",
        "    actualTopk_rs  = calcRelevanceScore(actualTopkDict, fixedIndexAFDDict, k)\n",
        "\n",
        "    # Algorithm 1 : LPDS\n",
        "    startKTimeLPDS = time.time()\n",
        "    patkGraph_lpds, ndcgkGraph_lpds = [], []\n",
        "\n",
        "    mainUBDict, mainLBDict, lpdsTimeVsDataDict, boundsTime2D, boundsTime3D, boundsTime4D = calculateBoundsLPDS(file_name, dataInterval, stopAtIndex)\n",
        "    keys = mainUBDict.keys()\n",
        "\n",
        "    for key in keys:\n",
        "\n",
        "      ubdataDict = mainUBDict[key]\n",
        "      lbdataDict = mainLBDict[key]\n",
        "      attr = list(ubdataDict.keys())\n",
        "\n",
        "      approxTopk_lpds = cal_mean(attr, ubdataDict, lbdataDict, e)\n",
        "      approxTopk_rs = adjustRelevanceScore(approxTopk_lpds, actualTopk[:k], actualTopk_rs[:k])\n",
        "\n",
        "      patkGraph_lpds.append(precision_at_k(actualTopk, approxTopk_lpds, k=k))\n",
        "      ndcgkGraph_lpds.append(ndcg_at_k(approxTopk_rs, actualTopk_rs, k=k))\n",
        "\n",
        "    stopKTimeLPDS = time.time()\n",
        "    lpdsTimeVsKDict[k] = stopKTimeLPDS - startKTimeLPDS\n",
        "\n",
        "    # Algorithm 2 : IPF\n",
        "    startKTimeIPF = time.time()\n",
        "    patkGraph_ipf, ndcgkGraph_ipf = [], []\n",
        "\n",
        "    mainIPFDict, ipfTimeVsDataDict = calculateIPF(file_name, dataInterval, stopAtIndex)\n",
        "    keys = mainIPFDict.keys()\n",
        "\n",
        "    for key in keys:\n",
        "\n",
        "      ipfDict = mainIPFDict[key]\n",
        "      attr = list(ipfDict.keys())\n",
        "\n",
        "      ipfTopkDict = {k: v for k, v in sorted(ipfDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "      approxTopk_ipf = sorted(ipfTopkDict, key=lambda x: ipfTopkDict[x])\n",
        "      ipfTopk_rs = adjustRelevanceScore(approxTopk_ipf, actualTopk[:k], actualTopk_rs[:k])\n",
        "\n",
        "      patkGraph_ipf.append(precision_at_k(actualTopk, approxTopk_ipf, k=k))\n",
        "      ndcgkGraph_ipf.append(ndcg_at_k(ipfTopk_rs, actualTopk_rs, k=k))\n",
        "\n",
        "    stopKTimeIPF = time.time()\n",
        "    ipfTimeVsKDict[k] = stopKTimeIPF - startKTimeIPF\n",
        "\n",
        "    # Algorithm 3 : Baseline\n",
        "    startKTimeBL = time.time()\n",
        "    patkGraph_bl, ndcgkGraph_bl = [], []\n",
        "\n",
        "    mainBLDict = calculateBaseline(file_name, dataInterval, stopAtIndex)\n",
        "    keys = mainBLDict.keys()\n",
        "\n",
        "    for key in keys:\n",
        "\n",
        "      blDict = mainBLDict[key]\n",
        "      attr = list(blDict.keys())\n",
        "\n",
        "      blTopkDict = {k: v for k, v in sorted(blDict.items(), key=lambda item: item[1]) if v <= e}\n",
        "      approxTopk_bl = sorted(blTopkDict, key=lambda y: blTopkDict[y])\n",
        "      blTopk_rs = adjustRelevanceScore(approxTopk_bl, actualTopk[:k], actualTopk_rs[:k])\n",
        "\n",
        "      patkGraph_bl.append(precision_at_k(actualTopk, approxTopk_bl, k=k))\n",
        "      ndcgkGraph_bl.append(ndcg_at_k(blTopk_rs, actualTopk_rs, k=k))\n",
        "\n",
        "    stopKTimeBL = time.time()\n",
        "    blTimeVsKDict[k] = stopKTimeBL - startKTimeBL\n",
        "\n",
        "    # Algorithm 4 : Pyro++\n",
        "    patkGraph_pyro, ndcgkGraph_pyro = [], []\n",
        "    patkGraph_pyro, ndcgkGraph_pyro = calculatePyroPatkAndNDCG(file_name, actualTopk, actualDict, actualTopk_rs, k)\n",
        "\n",
        "    # p@k and ndcg graphs\n",
        "    generatePatK(patkGraph_lpds, patkGraph_ipf, patkGraph_bl, patkGraph_pyro, rows, dataInterval, k, stopAtIndex, fn, sortedStr)\n",
        "    generateNDCGK(ndcgkGraph_lpds, ndcgkGraph_ipf, ndcgkGraph_bl, ndcgkGraph_pyro, rows, dataInterval, k, stopAtIndex, fn, sortedStr)\n",
        "\n",
        "  # clocktime graphs\n",
        "  generateKClocktimeGraph(lpdsTimeVsKDict, ipfTimeVsKDict, blTimeVsKDict, kList, fn, sortedStr)\n",
        "  generateDataClocktimeGraph(lpdsTimeVsDataDict, ipfTimeVsDataDict, rows, dataInterval, stopAtIndex, fn, sortedStr)\n",
        "  generateLPDSBoundsClocktimeGraph(boundsTime2D, boundsTime3D, boundsTime4D, rows, dataInterval, stopAtIndex, fn, sortedStr)\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y95GD2Fplnw"
      },
      "source": [
        "**Precision at k Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ApGKlC9dwkp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generatePatK(patkGraph_lpds, patkGraph_ipf, patkGraph_bl, patkGraph_pyro, rows, dataInterval, k, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bar_width = 0.15\n",
        "  index = np.arange(len(rows_read))\n",
        "\n",
        "  patterns = ['\\\\', '.', '-', 'x']\n",
        "\n",
        "  plt.bar(index, patkGraph_lpds, bar_width, label='AppxLP', hatch=patterns[0], facecolor='skyblue', edgecolor='black')\n",
        "  plt.bar(index + bar_width, patkGraph_ipf, bar_width, label='AppxIPF', hatch=patterns[1], facecolor='khaki', edgecolor='black')\n",
        "  plt.bar(index + 2 * bar_width, patkGraph_bl, bar_width, label='TANE', hatch=patterns[2], facecolor='darkseagreen', edgecolor='black')\n",
        "  plt.bar(index + 3 * bar_width, patkGraph_pyro, bar_width, label='Pyro++', hatch=patterns[3], facecolor='lightsalmon', edgecolor='black')\n",
        "\n",
        "  plt.xlabel('Percent of Data Read', fontsize=16, fontweight='bold')\n",
        "  plt.ylabel('Precision', fontsize=16, fontweight='bold')\n",
        "\n",
        "  plt.xticks(index + bar_width * 1.75, [f'{round(r, 0)}' for r in rows_read], fontsize=16)\n",
        "  plt.yticks(fontsize=16)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.22), ncol=2, prop={'weight': 'bold', 'size': 14})\n",
        "\n",
        "  plt.savefig('P@K=' + str(k) + '_' + fn + '_' + sortedStr + '.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVIYTsh_akd3"
      },
      "source": [
        "**NDCG Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP57YclHenL6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generateNDCGK(ndcgkGraph_lpds, ndcgkGraph_ipf, ndcgkGraph_bl, ndcgkGraph_pyro, rows, dataInterval, k, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bar_width = 0.15\n",
        "  index = np.arange(len(rows_read))\n",
        "\n",
        "  patterns = ['\\\\', '.', '-', 'x']\n",
        "\n",
        "  plt.bar(index, ndcgkGraph_lpds, bar_width, label='AppxLP', hatch=patterns[0], facecolor='skyblue', edgecolor='black')\n",
        "  plt.bar(index + bar_width, ndcgkGraph_ipf, bar_width, label='AppxIPF', hatch=patterns[1], facecolor='khaki', edgecolor='black')\n",
        "  plt.bar(index + 2 * bar_width, ndcgkGraph_bl, bar_width, label='TANE', hatch=patterns[2], facecolor='darkseagreen', edgecolor='black')\n",
        "  plt.bar(index + 3 * bar_width, ndcgkGraph_pyro, bar_width, label='Pyro++', hatch=patterns[3], facecolor='lightsalmon', edgecolor='black')\n",
        "\n",
        "  plt.xlabel('Percent of Data Read', fontsize=16, fontweight='bold')\n",
        "  plt.ylabel('NDCG', fontsize=16, fontweight='bold')\n",
        "\n",
        "  plt.xticks(index + bar_width * 1.75, [f'{round(r, 0)}' for r in rows_read], fontsize=16)\n",
        "  plt.yticks(fontsize=16)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.22), ncol=2, prop={'weight': 'bold', 'size': 14})\n",
        "\n",
        "  plt.savefig('NDCG@K=' + str(k) + '_' + fn + '_' + sortedStr + '.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge_VcQLHej_e"
      },
      "source": [
        "**K vs Clocktime Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC_Id1urah3_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateKClocktimeGraph(lpdsTimeVsKDict, ipfTimeVsKDict, blTimeVsKDict, kList, fn, sortedStr):\n",
        "\n",
        "  x = kList\n",
        "  y1 = list(lpdsTimeVsKDict.values())\n",
        "  y2 = list(ipfTimeVsKDict.values())\n",
        "\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  plt.plot(x, y1, marker='o', linestyle='-', color='dodgerblue', label='AppxLP', linewidth=5)\n",
        "  plt.plot(x, y2, marker='x', linestyle='-', color='gold', label='AppxIPF', linewidth=5)\n",
        "\n",
        "  plt.xticks(x, fontsize=35)\n",
        "  y_min = min(min(y1), min(y2)) - 0.05\n",
        "  y_max = max(max(y1), max(y2)) + 0.05\n",
        "  yticks = np.round(np.linspace(y_min, y_max, num=5), 3)\n",
        "\n",
        "  plt.yticks(yticks, fontsize=35)\n",
        "\n",
        "  plt.xlabel('k', fontsize=34, fontweight='bold')\n",
        "  plt.ylabel('Time (sec)', fontsize=34, fontweight='bold')\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.20), ncol=2, prop={'weight': 'bold', 'size': 35})\n",
        "\n",
        "  plt.savefig('AppxKvsClocktime'+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpRgfMOmJUW"
      },
      "source": [
        "**Data Read vs Clocktime Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKXj7ulJmEq-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateDataClocktimeGraph(lpdsTimeVsDataDict, ipfTimeVsDataDict, rows, dataInterval, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  y1 = list(lpdsTimeVsDataDict.values())\n",
        "  y2 = list(ipfTimeVsDataDict.values())\n",
        "\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  plt.plot(rows_read, y1, marker='o', linestyle='-', color='dodgerblue', label='AppxLP', linewidth=5)\n",
        "  plt.plot(rows_read, y2, marker='x', linestyle='-', color='gold', label='AppxIPF', linewidth=5)\n",
        "\n",
        "  plt.xticks(rows_read, fontsize=35)\n",
        "  y_min = min(min(y1), min(y2)) - 0.001\n",
        "  y_max = max(max(y1), max(y2)) + 0.001\n",
        "  yticks = np.round(np.linspace(y_min, y_max, num=6), 4)\n",
        "  plt.yticks(yticks, fontsize=35)\n",
        "\n",
        "  plt.xlabel('Data Read', fontsize=34, fontweight='bold')\n",
        "  plt.ylabel('Time (sec)', fontsize=34, fontweight='bold')\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.20), ncol=2, prop={'weight': 'bold', 'size': 35})\n",
        "\n",
        "  plt.savefig('AppxDatavsClocktime'+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7bg84rBoE4"
      },
      "source": [
        "**Bounds vs Clocktime Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv8VtjoPBnRT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateLPDSBoundsClocktimeGraph(boundsTime2D, boundsTime3D, boundsTime4D, rows, dataInterval, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bounds2D = list(boundsTime2D.values())\n",
        "  bounds3D = list(boundsTime3D.values())\n",
        "  bounds4D = list(boundsTime4D.values())\n",
        "\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  plt.plot(rows_read, bounds2D, marker='^', linestyle='-', color='black', label='One LHS AFD')\n",
        "  plt.plot(rows_read, bounds3D, marker='D', linestyle='-', color='black', label='Two LHS AFD')\n",
        "  plt.plot(rows_read, bounds4D, marker='s', linestyle='-', color='black', label='Three LHS AFD')\n",
        "\n",
        "  plt.xticks(fontsize=14)\n",
        "  plt.yticks(fontsize=14)\n",
        "\n",
        "  plt.xlabel('Data Read', fontsize=20, fontweight='bold')\n",
        "  plt.ylabel('Time (sec)', fontsize=20, fontweight='bold')\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, prop={'weight': 'bold', 'size': 20})\n",
        "\n",
        "  plt.savefig('AppxBoundsvsClocktime'+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHoOSJ4Yo8mn"
      },
      "source": [
        "**Dataset Sorting (Key/Non-Key)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPILjgoQCUFq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sortDataset(file_name, attrList, attr, sortOrder):\n",
        "\n",
        "  if attr != '0':\n",
        "    df = pd.read_csv(file_name)\n",
        "    df_sorted = df.sort_values(by=attrList, ascending=sortOrder)\n",
        "    sortedFN = 'Sorted_X'+attr+'_'+file_name\n",
        "    df_sorted.to_csv(sortedFN, index=False)\n",
        "  else:\n",
        "    return file_name\n",
        "\n",
        "  return sortedFN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk-h_e7Kpq6l"
      },
      "source": [
        "**Start Point** (Define parameters and run code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bs2R_WWoyfD8"
      },
      "outputs": [],
      "source": [
        "def getDatasetParam(datasetName):\n",
        "\n",
        "    dataset_config = {\n",
        "        'dataset1': {'fn': 'DB Status', 'rows': 10000, 'interval': 1000, 'fileName': 'DBStatus.csv'},\n",
        "        'dataset2': {'fn': 'Letter', 'rows': 20000, 'interval': 2000, 'fileName': 'Letter.csv'},\n",
        "        'dataset3': {'fn': 'Student', 'rows': 1000, 'interval': 100, 'fileName': 'Student.csv'},\n",
        "        'dataset4': {'fn': 'Synthetic1', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData1.csv'},\n",
        "        'dataset5': {'fn': 'Synthetic2', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData2.csv'},\n",
        "        'dataset6': {'fn': 'Synthetic3', 'rows': 160000, 'interval': 16000, 'fileName': 'SyntheticData3.csv'},\n",
        "        'dataset7': {'fn': 'Synthetic4', 'rows': 48000, 'interval': 4800, 'fileName': 'SyntheticData4.csv'}\n",
        "    }\n",
        "\n",
        "    if datasetName in dataset_config:\n",
        "        return dataset_config[datasetName]\n",
        "    else:\n",
        "        raise ValueError(f\"{datasetName} not found.\")\n",
        "\n",
        "def AFDStartPoint():\n",
        "\n",
        "  # define dataset paramters\n",
        "  datasetNum = 'dataset1'\n",
        "  params = getDatasetParam(datasetNum)\n",
        "  fn = params['fn']\n",
        "  rows = params['rows']\n",
        "  interval = params['interval']\n",
        "  fileName = params['fileName']\n",
        "\n",
        "  # define sorting order\n",
        "  attr = '0'\n",
        "  attrList = ['0']\n",
        "  sortOrder = [True]\n",
        "  sortedStr = ''\n",
        "  data = sortDataset(fileName, attrList, attr, sortOrder)\n",
        "\n",
        "  # threshold used by pyro\n",
        "  e = 0.01\n",
        "  # define k values\n",
        "  kList = [3,5,7]\n",
        "  # define limit of records read\n",
        "  stopAtIndex = 0.6*rows\n",
        "\n",
        "  # pass the parameters and generate results\n",
        "  calculatePatKandNDCG(data, interval, rows, kList, stopAtIndex, fn, sortedStr, e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpTdkGqBJMi4",
        "outputId": "bb9bae5d-5e11-4945-f60c-a9180a6df719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4D Actual Conflict {'123': 0.009844234423442344, '124': 0.010122462246224623, '125': 0.010172777277727772, '126': 0.010645844584458447, '134': 0.009526092609260926, '135': 0.009858185818581859, '136': 0.009692269226922692, '145': 0.009831793179317932, '146': 0.009534183418341835, '156': 0.009887938793879388, '234': 0.009508060806080607, '235': 0.00984015401540154, '236': 0.00968927892789279, '245': 0.009777697769776978, '246': 0.009511591159115911, '256': 0.009865406540654065, '345': 0.009522912291229122, '346': 0.009368646864686468, '356': 0.009691269126912691, '456': 0.009513861386138614}\n",
            "3D Actual Conflict {'12': 0.02242954295429543, '13': 0.009862266226622663, '14': 0.010212621262126213, '15': 0.010226872687268728, '16': 0.010670737073707371, '23': 0.009844234423442344, '24': 0.010122462246224623, '25': 0.010172777277727772, '26': 0.010645844584458447, '34': 0.009526092609260926, '35': 0.009858185818581859, '36': 0.009692269226922692, '45': 0.009831793179317932, '46': 0.009534183418341835, '56': 0.009887938793879388}\n",
            "2D Actual Conflict {'1': 0.023902140214021404, '2': 0.02242954295429543, '3': 0.009862266226622663, '4': 0.010212621262126213, '5': 0.010226872687268728, '6': 0.010670737073707371}\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "./logFiles/a_Extracted_PDBX_10000_first_row.log\n",
            "./logFiles/b_extracted_PDBX_10000_10percent.log\n",
            "./logFiles/c_extracted_PDBX_10000_20percent.log\n",
            "./logFiles/d_extracted_PDBX_10000_30percent.log\n",
            "./logFiles/e_extracted_PDBX_10000_40percent.log\n",
            "./logFiles/f_extracted_PDBX_10000_50percent.log\n",
            "./logFiles/g_extracted_PDBX_10000_60percent.log\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "./logFiles/a_Extracted_PDBX_10000_first_row.log\n",
            "./logFiles/b_extracted_PDBX_10000_10percent.log\n",
            "./logFiles/c_extracted_PDBX_10000_20percent.log\n",
            "./logFiles/d_extracted_PDBX_10000_30percent.log\n",
            "./logFiles/e_extracted_PDBX_10000_40percent.log\n",
            "./logFiles/f_extracted_PDBX_10000_50percent.log\n",
            "./logFiles/g_extracted_PDBX_10000_60percent.log\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "Maximum iterations reached\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
            "./logFiles/a_Extracted_PDBX_10000_first_row.log\n",
            "./logFiles/b_extracted_PDBX_10000_10percent.log\n",
            "./logFiles/c_extracted_PDBX_10000_20percent.log\n",
            "./logFiles/d_extracted_PDBX_10000_30percent.log\n",
            "./logFiles/e_extracted_PDBX_10000_40percent.log\n",
            "./logFiles/f_extracted_PDBX_10000_50percent.log\n",
            "./logFiles/g_extracted_PDBX_10000_60percent.log\n"
          ]
        }
      ],
      "source": [
        "# start point\n",
        "AFDStartPoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}