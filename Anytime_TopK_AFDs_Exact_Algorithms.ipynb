{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_MhVcpyrBAw"
      },
      "source": [
        "**Upper Bound - LP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pulp\n",
        "!pip install gurobipy\n",
        "\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from typing import List\n",
        "\n",
        "def get_ub_two_dims(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "    n, m = len(row_sum), len(col_sum)\n",
        "\n",
        "    model = gp.Model(\"Upper_Bound_Problem\")\n",
        "    model.setParam(\"OutputFlag\", 0)\n",
        "\n",
        "    # Variables\n",
        "    matrix = model.addVars(n, m, name=\"u\", lb=0.0)\n",
        "    row_max = model.addVars(n, name=\"rmax\", lb=0.0)\n",
        "\n",
        "    # Objective: Minimize sum of row maxima\n",
        "    model.setObjective(gp.quicksum(row_max[i] for i in range(n)), GRB.MINIMIZE)\n",
        "\n",
        "    # Row constraints: sum of unseen + seen equals row sum\n",
        "    for i in range(n):\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix[i, j] for j in range(m)) + sum(seen[i][j] for j in range(m)) == row_sum[i]\n",
        "        )\n",
        "\n",
        "    # Column constraints: sum of unseen + seen â‰¤ column sum\n",
        "    for j in range(m):\n",
        "        model.addConstr(\n",
        "            gp.quicksum(matrix[i, j] for i in range(n)) + sum(seen[i][j] for i in range(n)) <= col_sum[j]\n",
        "        )\n",
        "\n",
        "    # Row max constraints\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            model.addConstr(row_max[i] >= matrix[i, j] + seen[i][j])\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.Status != GRB.OPTIMAL:\n",
        "        raise RuntimeError(\"Gurobi did not find an optimal solution.\")\n",
        "\n",
        "    total_row_max = sum(row_max[i].X for i in range(n))\n",
        "    conflict = int(sum(row_sum) - total_row_max)\n",
        "    return conflict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8qH8tcfSRQ3",
        "outputId": "22ec0557-47f6-4fef-e725-1f50646c2913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pulp in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (12.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVt0GPXrHAg"
      },
      "source": [
        "**Lower Bound - Mixed LP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from typing import List\n",
        "\n",
        "def get_lb_two_dims(seen: List[List[int]], row_sum: List[int], col_sum: List[int]) -> int:\n",
        "    conflict = 0\n",
        "\n",
        "    if seen and row_sum:\n",
        "        n = len(row_sum)\n",
        "        m = len(col_sum)\n",
        "\n",
        "        row_sum_seen = list(map(sum, seen))\n",
        "        col_sum_seen = list(map(sum, zip(*seen)))\n",
        "\n",
        "        row_max_seen = list(map(max, seen))\n",
        "        col_max_seen = list(map(max, zip(*seen)))\n",
        "\n",
        "        diff_row_sum = [row_sum[i] - row_sum_seen[i] for i in range(n)]\n",
        "        diff_col_sum = [col_sum[j] - col_sum_seen[j] for j in range(m)]\n",
        "\n",
        "        upper_bound_row_sum = [\n",
        "            max(seen[i][j] + min(diff_row_sum[i], diff_col_sum[j]) for j in range(m))\n",
        "            for i in range(n)\n",
        "        ]\n",
        "\n",
        "        model = gp.Model(\"Lower_Bound_Problem\")\n",
        "        model.setParam('OutputFlag', 0)\n",
        "\n",
        "        matrix = model.addVars(n, m, name=\"unseen\", lb=0.0)\n",
        "        row_max = model.addVars(n, name=\"row_max\", lb=0.0)\n",
        "        selected = model.addVars(n, m, name=\"selected\", lb=0.0)\n",
        "\n",
        "        for i in range(n):\n",
        "            model.addConstr(row_max[i] <= upper_bound_row_sum[i])\n",
        "            model.addConstr(gp.quicksum(matrix[i, j] for j in range(m)) + sum(seen[i][j] for j in range(m)) == row_sum[i])\n",
        "            model.addConstr(gp.quicksum(selected[i, j] for j in range(m)) == 1)\n",
        "\n",
        "        for j in range(m):\n",
        "            model.addConstr(gp.quicksum(matrix[i, j] for i in range(n)) + sum(seen[i][j] for i in range(n)) <= col_sum[j])\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                model.addConstr(row_max[i] >= seen[i][j] + matrix[i, j])\n",
        "                model.addConstr(\n",
        "                    row_max[i] <= seen[i][j] + matrix[i, j] +\n",
        "                    (row_sum[i] - row_sum_seen[i] + row_max_seen[i] - seen[i][j]) * (1 - selected[i, j])\n",
        "                )\n",
        "\n",
        "        model.setObjective(gp.quicksum(row_max[i] for i in range(n)), GRB.MAXIMIZE)\n",
        "        model.optimize()\n",
        "\n",
        "        if model.Status == GRB.OPTIMAL:\n",
        "            total_row_max = sum(row_max[i].X for i in range(n))\n",
        "            conflict = int(sum(row_sum) - total_row_max)\n",
        "\n",
        "    return conflict"
      ],
      "metadata": {
        "id": "l1032F0sULwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWqtdi9rNUe"
      },
      "source": [
        "**Lower Bound - Disregard Column Sum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR3kNCPTvi_5"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import List, Set, Tuple\n",
        "\n",
        "def lb_disregardColSum_2D(seen: List[List[int]], row_sum_orig: List[int], col_sum_orig: List[int])-> int:\n",
        "\n",
        "  row_sum = copy.copy(row_sum_orig)\n",
        "  col_sum = copy.copy(col_sum_orig)\n",
        "  conflict = sum(row_sum)\n",
        "  n = len(row_sum)\n",
        "  m = len(col_sum)\n",
        "  seen_row_sum = [0] * n\n",
        "  seen_col_sum = [0] * m\n",
        "  remaining_col = [0] * m\n",
        "  remaining_row = [0] * n\n",
        "  for i in range(n):\n",
        "    for j in range(m):\n",
        "      seen_row_sum[i] += seen[i][j]\n",
        "      seen_col_sum[j] += seen[i][j]\n",
        "    remaining_row[i] = row_sum[i] - seen_row_sum[i]\n",
        "\n",
        "  for j in range(m):\n",
        "    remaining_col[j] = col_sum[j] - seen_col_sum[j]\n",
        "\n",
        "  for i in range(n):\n",
        "    for j in range(m):\n",
        "      if j==0:\n",
        "        max_col = seen[i][j] + min(remaining_col[j], remaining_row[i])\n",
        "      else:\n",
        "        cur_entry = seen[i][j] + min(remaining_col[j], remaining_row[i])\n",
        "        if max_col < cur_entry:\n",
        "          max_col = cur_entry\n",
        "    conflict = conflict - max_col\n",
        "\n",
        "  return conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3OcGugkrmi7"
      },
      "source": [
        "**Exact Top-K Ranking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FQ00tY1WoYw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def calculateExactTopk(LB, UB):\n",
        "\n",
        "  exactTopK = []\n",
        "  remainingAFD = set(UB.keys())\n",
        "\n",
        "  while remainingAFD:\n",
        "    isTopk = False\n",
        "    for ubAFD in list(remainingAFD):\n",
        "      ubVal = UB[ubAFD]\n",
        "      if all(ubVal < LB[lbAFD] for lbAFD in remainingAFD if lbAFD != ubAFD):\n",
        "        exactTopK.append(ubAFD)\n",
        "        remainingAFD.remove(ubAFD)\n",
        "        isTopk = True\n",
        "        break\n",
        "    if not isTopk:\n",
        "      break\n",
        "\n",
        "  exactTopK = [int(afd) for afd in exactTopK]\n",
        "\n",
        "  return exactTopK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMTcJ1qDsO_X"
      },
      "source": [
        "**Actual Top-K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J05xQ4sSN0I0"
      },
      "outputs": [],
      "source": [
        "def calculateActualTopk(file_name):\n",
        "\n",
        "  actualConflictDict = {}\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "    conflict = 0\n",
        "    for index,row in enumerate(matrix):\n",
        "      conflict += row_sum[index]-max(row)\n",
        "\n",
        "    actualConflictDict[A_name] = conflict\n",
        "\n",
        "  actualTopk = sorted(actualConflictDict, key=lambda k: actualConflictDict[k])\n",
        "  actualTopk = [int(key) for key in actualTopk]\n",
        "\n",
        "  actualTopkDict = dict(sorted(actualConflictDict.items(), key=lambda item: item[1]))\n",
        "\n",
        "  return actualTopk, actualTopkDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculateActualG3Error(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  actualErrorFile = 'ActualG3Error.xlsx'\n",
        "\n",
        "  df_empty = pd.DataFrame()\n",
        "  df_empty.to_excel(actualErrorFile, index=False)\n",
        "\n",
        "  isIndex = 0\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    oldActualDF = pd.read_excel(actualErrorFile)\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    matrix = []\n",
        "    for i in a_indices:\n",
        "      matrix.append([0]*len(z_indices))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "\n",
        "    indexDict = {'Index': []}\n",
        "    actualG3Dict = {A_name: []}\n",
        "\n",
        "    for dataIDX,data in enumerate(dataset):\n",
        "\n",
        "      if dataIDX > stopAtIndex:\n",
        "        break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "      matrix[row_index][col_index] += 1\n",
        "\n",
        "      conflict = 0\n",
        "      for index,row in enumerate(matrix):\n",
        "        conflict += row_sum[index]-max(row)\n",
        "\n",
        "      if dataIDX % dataInterval == 0 or dataIDX+1 == len(dataset):\n",
        "        indexDict['Index'].append(dataIDX+1)\n",
        "        actualG3Dict[A_name].append(conflict)\n",
        "\n",
        "    dfIndex = pd.DataFrame(indexDict)\n",
        "    dfActual = pd.DataFrame(actualG3Dict)\n",
        "\n",
        "    if isIndex == 0:\n",
        "      oldActualDF = pd.concat([oldActualDF, dfIndex], axis=1, ignore_index=False)\n",
        "      isIndex = 1\n",
        "    dfActual = pd.concat([oldActualDF, dfActual], axis=1, ignore_index=False)\n",
        "\n",
        "    dfActual.to_excel(actualErrorFile, index=False)"
      ],
      "metadata": {
        "id": "rsFMvdwUod_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYAvL8msS4N"
      },
      "source": [
        "**Calculating Upper and Lower Bounds**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def removeSingleMarginals(calc_matrix, row_sum, col_sum):\n",
        "\n",
        "  row_sum_copy = row_sum[:]\n",
        "  col_sum_copy = col_sum[:]\n",
        "  matrix_copy = [row[:] for row in calc_matrix]\n",
        "\n",
        "  rows_to_remove = [i for i, rsum in enumerate(row_sum_copy) if rsum == 1]\n",
        "\n",
        "  for i in sorted(rows_to_remove, reverse=True):\n",
        "    row = matrix_copy[i]\n",
        "    for j, val in enumerate(row):\n",
        "        col_sum_copy[j] -= val\n",
        "\n",
        "    del matrix_copy[i]\n",
        "    del row_sum_copy[i]\n",
        "\n",
        "  return matrix_copy, row_sum_copy, col_sum_copy"
      ],
      "metadata": {
        "id": "X4ksN3lyEv_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZQKcYUsDaaD"
      },
      "source": [
        "**Algorithm 1: Exact LP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-ssfoCh3dA-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculateBoundsMLP(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  UBResultFile = 'UBResults.xlsx'\n",
        "  LB1ResultFile = 'MLP_LBResults.xlsx'\n",
        "\n",
        "  df_empty = pd.DataFrame()\n",
        "  df_empty.to_excel(UBResultFile, index=False)\n",
        "\n",
        "  df_empty = pd.DataFrame()\n",
        "  df_empty.to_excel(LB1ResultFile, index=False)\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  isIndex = 0\n",
        "\n",
        "  lpUBDict = {}\n",
        "  lpLBDict = {}\n",
        "  boundsTimeLP = {}\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    oldUBDF = pd.read_excel(UBResultFile)\n",
        "    oldLB1DF = pd.read_excel(LB1ResultFile)\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    op1 = []\n",
        "    calc_matrix = []\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    indexDict = {'Index': []}\n",
        "    ubdataDict = {A_name: []}\n",
        "    lb1dataDict = {A_name: []}\n",
        "\n",
        "    ubTimeDictLP = {}\n",
        "    lbTimeDictLP = {}\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "          break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        if index not in lpUBDict:\n",
        "          lpUBDict[index] = {}\n",
        "\n",
        "        if index not in lpLBDict:\n",
        "          lpLBDict[index] = {}\n",
        "\n",
        "        mat, rowS, colS = removeSingleMarginals(calc_matrix, row_sum, col_sum)\n",
        "\n",
        "        startTimeLP = time.time()\n",
        "\n",
        "        startUBTimeLP = time.time()\n",
        "        ubValue = get_ub_two_dims(mat, rowS, colS)\n",
        "        stopUBTimeLP = time.time()\n",
        "\n",
        "        startLBTimeLP = time.time()\n",
        "        lb1Value = get_lb_two_dims(mat, rowS, colS)\n",
        "        stopLBTimeLP = time.time()\n",
        "\n",
        "        stopTimeLP = time.time()\n",
        "\n",
        "        boundsTimeLP[index] = stopTimeLP - startTimeLP\n",
        "\n",
        "        ubTimeDictLP[index] = stopUBTimeLP - startUBTimeLP\n",
        "        lbTimeDictLP[index] = stopLBTimeLP - startLBTimeLP\n",
        "\n",
        "        if not op1:\n",
        "          op1.append([index+1, ubValue, lb1Value])\n",
        "          u1 = ubValue\n",
        "          l1 = lb1Value\n",
        "\n",
        "        else:\n",
        "          op1.append([index+1, min(ubValue, op1[-1][1]), max(lb1Value, op1[-1][2])])\n",
        "          u1 = min(ubValue, op1[-1][1])\n",
        "          l1 = max(lb1Value, op1[-1][2])\n",
        "\n",
        "        lpUBDict[index][A_name] = u1\n",
        "        lpLBDict[index][A_name] = l1\n",
        "\n",
        "        indexDict['Index'].append(index+1)\n",
        "        ubdataDict[A_name].append(u1)\n",
        "        lb1dataDict[A_name].append(l1)\n",
        "\n",
        "    dfIndex = pd.DataFrame(indexDict)\n",
        "    dfUB = pd.DataFrame(ubdataDict)\n",
        "    dfLB1 = pd.DataFrame(lb1dataDict)\n",
        "\n",
        "    if isIndex == 0:\n",
        "      oldUBDF = pd.concat([oldUBDF, dfIndex], axis=1, ignore_index=False)\n",
        "      oldLB1DF = pd.concat([oldLB1DF, dfIndex], axis=1, ignore_index=False)\n",
        "      isIndex = 1\n",
        "\n",
        "    dfUB = pd.concat([oldUBDF, dfUB], axis=1, ignore_index=False)\n",
        "    dfLB1 = pd.concat([oldLB1DF, dfLB1], axis=1, ignore_index=False)\n",
        "\n",
        "    dfUB.to_excel(UBResultFile, index=False)\n",
        "    dfLB1.to_excel(LB1ResultFile, index=False)\n",
        "\n",
        "    df2 = pd.DataFrame(list(ubTimeDictLP.items()), columns=['Index', 'Time'])\n",
        "    df3 = pd.DataFrame(list(lbTimeDictLP.items()), columns=['Index', 'Time'])\n",
        "\n",
        "    df2.to_excel('ExUBTimeLP.xlsx', index=False)\n",
        "    df3.to_excel('ExLBTimeLP.xlsx', index=False)\n",
        "\n",
        "  return lpUBDict, lpLBDict, boundsTimeLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leexINJlDUMO"
      },
      "source": [
        "**Algorithm 2: Exact DCS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0F2aWVnDAb-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculateBoundsDCS(file_name, dataInterval, stopAtIndex):\n",
        "\n",
        "  UBResultFile = 'UBResults.xlsx'\n",
        "  LB2ResultFile = 'DCS_LBResults.xlsx'\n",
        "\n",
        "  df_empty = pd.DataFrame()\n",
        "  df_empty.to_excel(UBResultFile, index=False)\n",
        "\n",
        "  df_empty = pd.DataFrame()\n",
        "  df_empty.to_excel(LB2ResultFile, index=False)\n",
        "\n",
        "  mainDF = pd.read_csv(file_name, header = 0)\n",
        "  # last column in dataset is the Z attribute\n",
        "  A_name_list = mainDF.columns[:-1].tolist()\n",
        "\n",
        "  isIndex = 0\n",
        "\n",
        "  dcsUBDict = {}\n",
        "  dcsLBDict = {}\n",
        "  boundsTimeDCS = {}\n",
        "\n",
        "  for A_name in A_name_list:\n",
        "\n",
        "    oldUBDF = pd.read_excel(UBResultFile)\n",
        "    oldLB2DF = pd.read_excel(LB2ResultFile)\n",
        "\n",
        "    Z_name = \"y\"\n",
        "    df = mainDF[[A_name, Z_name]]\n",
        "\n",
        "    a_indices = np.unique(df[A_name])\n",
        "    a_indices = a_indices.tolist()\n",
        "    a_indices_map = dict(zip(a_indices, range(len(a_indices))))\n",
        "\n",
        "    z_indices = np.unique(df[Z_name])\n",
        "    z_indices = z_indices.tolist()\n",
        "    z_indices_map = dict(zip(z_indices, range(len(z_indices))))\n",
        "\n",
        "    dataset = df.values.tolist()\n",
        "    row_sum = [0]*len(a_indices)\n",
        "    col_sum = [0]*len(z_indices)\n",
        "\n",
        "    for data in dataset:\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      row_sum[row_index] += 1\n",
        "      col_sum[col_index] += 1\n",
        "\n",
        "    op1 = []\n",
        "    calc_matrix = []\n",
        "\n",
        "    for i in a_indices:\n",
        "      calc_matrix.append([0]*len(z_indices))\n",
        "\n",
        "    indexDict = {'Index': []}\n",
        "    ubdataDict = {A_name: []}\n",
        "    lb2dataDict = {A_name: []}\n",
        "\n",
        "    ubTimeDictDCS = {}\n",
        "    lbTimeDictDCS = {}\n",
        "\n",
        "    for index,data in enumerate(dataset):\n",
        "\n",
        "      if index > stopAtIndex:\n",
        "          break\n",
        "\n",
        "      row_index = a_indices_map[data[0]]\n",
        "      col_index = z_indices_map[data[1]]\n",
        "      calc_matrix[row_index][col_index] += 1\n",
        "\n",
        "      if index % dataInterval == 0 or index == len(dataset)-1:\n",
        "\n",
        "        if index not in dcsUBDict:\n",
        "          dcsUBDict[index] = {}\n",
        "\n",
        "        if index not in dcsLBDict:\n",
        "          dcsLBDict[index] = {}\n",
        "\n",
        "        mat, rowS, colS = removeSingleMarginals(calc_matrix, row_sum, col_sum)\n",
        "\n",
        "        startTimeDCS = time.time()\n",
        "\n",
        "        startUBTimeDCS = time.time()\n",
        "        ubValue = get_ub_two_dims(mat, rowS, colS)\n",
        "        stopUBTimeDCS = time.time()\n",
        "\n",
        "        startLBTimeDCS = time.time()\n",
        "        lb2Value = lb_disregardColSum_2D(mat, rowS, colS)\n",
        "        stopLBTimeDCS = time.time()\n",
        "\n",
        "        stopTimeDCS = time.time()\n",
        "\n",
        "        boundsTimeDCS[index] = stopTimeDCS - startTimeDCS\n",
        "\n",
        "        ubTimeDictDCS[index] = stopUBTimeDCS - startUBTimeDCS\n",
        "        lbTimeDictDCS[index] = stopLBTimeDCS - startLBTimeDCS\n",
        "\n",
        "        if not op1:\n",
        "          op1.append([index+1, ubValue, lb2Value])\n",
        "          u1 = ubValue\n",
        "          l2 = lb2Value\n",
        "        else:\n",
        "          op1.append([index+1, min(ubValue, op1[-1][1]), max(lb2Value, op1[-1][2])])\n",
        "          u1 = min(ubValue, op1[-1][1])\n",
        "          l2 = max(lb2Value, op1[-1][2])\n",
        "\n",
        "        dcsUBDict[index][A_name] = u1\n",
        "        dcsLBDict[index][A_name] = l2\n",
        "\n",
        "        indexDict['Index'].append(index+1)\n",
        "        ubdataDict[A_name].append(u1)\n",
        "        lb2dataDict[A_name].append(l2)\n",
        "\n",
        "    dfIndex = pd.DataFrame(indexDict)\n",
        "    dfUB = pd.DataFrame(ubdataDict)\n",
        "    dfLB2 = pd.DataFrame(lb2dataDict)\n",
        "\n",
        "    if isIndex == 0:\n",
        "      oldUBDF = pd.concat([oldUBDF, dfIndex], axis=1, ignore_index=False)\n",
        "      oldLB2DF = pd.concat([oldLB2DF, dfIndex], axis=1, ignore_index=False)\n",
        "      isIndex = 1\n",
        "\n",
        "    dfUB = pd.concat([oldUBDF, dfUB], axis=1, ignore_index=False)\n",
        "    dfLB2 = pd.concat([oldLB2DF, dfLB2], axis=1, ignore_index=False)\n",
        "\n",
        "    dfUB.to_excel(UBResultFile, index=False)\n",
        "    dfLB2.to_excel(LB2ResultFile, index=False)\n",
        "\n",
        "    df4 = pd.DataFrame(list(ubTimeDictDCS.items()), columns=['Index', 'Time'])\n",
        "    df5 = pd.DataFrame(list(lbTimeDictDCS.items()), columns=['Index', 'Time'])\n",
        "\n",
        "    df4.to_excel('ExUBTimeDCS.xlsx', index=False)\n",
        "    df5.to_excel('ExLBTimeDCS.xlsx', index=False)\n",
        "\n",
        "  return dcsUBDict, dcsLBDict, boundsTimeDCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nv40NLkrwdb"
      },
      "source": [
        "**Score Calculation - P@K/NDCG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHoXOPNIa2rp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def excelToDict(file_path):\n",
        "\n",
        "  df = pd.read_excel(file_path)\n",
        "  data_dict = {}\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    index_value = row['Index']\n",
        "    sub_dict = {column: row[column] for column in df.columns if column != 'Index'}\n",
        "    data_dict[index_value] = sub_dict\n",
        "\n",
        "  return data_dict\n",
        "\n",
        "# Precision at k score\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "\n",
        "  patk = 0.0\n",
        "\n",
        "  if y_pred:\n",
        "    k = min(k, len(y_pred))\n",
        "    relevantItems = sum([1 for item in y_pred[:k] if item in y_true[:k]])\n",
        "    patk = relevantItems/k\n",
        "\n",
        "  return patk\n",
        "\n",
        "def checkEqualAFDs(topkDict):\n",
        "\n",
        "  sortedKeysByVal = sorted(topkDict, key=topkDict.get)\n",
        "\n",
        "  eqIndexDict = {}\n",
        "  current_index = 0\n",
        "  prev_value = None\n",
        "\n",
        "  for key in sortedKeysByVal:\n",
        "      value = topkDict[key]\n",
        "      if value != prev_value:\n",
        "          current_index += 1\n",
        "      eqIndexDict[key] = current_index - 1\n",
        "      prev_value = value\n",
        "\n",
        "  return eqIndexDict\n",
        "\n",
        "# ndcg - true relevance score\n",
        "def calcRelevanceScore(topkDict, fixedIndexDict, k):\n",
        "\n",
        "  true_rs = [0] * len(fixedIndexDict)\n",
        "\n",
        "  fixedIndexDict = checkEqualAFDs(topkDict)\n",
        "\n",
        "  if topkDict:\n",
        "    maxValNorm = topkDict[max(topkDict, key=topkDict.get)]\n",
        "    for key, value in topkDict.items():\n",
        "      if maxValNorm != 0:\n",
        "        normVal_RS = 1-(value/maxValNorm)\n",
        "      else:\n",
        "        normVal_RS = 0\n",
        "\n",
        "      if key in fixedIndexDict:\n",
        "        true_rs[fixedIndexDict[key]] = normVal_RS\n",
        "\n",
        "  return true_rs\n",
        "\n",
        "# ndcg - predicted relevance score\n",
        "def adjustRelevanceScore(predTopk, trueTopk, true_rs):\n",
        "\n",
        "  relMap = dict(zip(trueTopk, true_rs))\n",
        "  predTopk = predTopk[:len(trueTopk)]\n",
        "\n",
        "  if predTopk:\n",
        "    pred_rs = [relMap.get(item, 0) for item in predTopk]\n",
        "    pred_rs += [0] * (len(trueTopk) - len(predTopk))\n",
        "  else:\n",
        "    pred_rs = [0] * len(trueTopk)\n",
        "\n",
        "  return pred_rs\n",
        "\n",
        "# dcg score\n",
        "def dcg_at_k(relevance_scores, k):\n",
        "\n",
        "  dcg = 0.0\n",
        "  for i in range(min(k, len(relevance_scores))):\n",
        "      dcg += (2**relevance_scores[i] - 1) / np.log2(i + 2)\n",
        "  return dcg\n",
        "\n",
        "# ncdg score\n",
        "def ndcg_at_k(pred_rel, true_rel, k):\n",
        "\n",
        "  dcg_max = dcg_at_k(pred_rel, k)\n",
        "  ideal_relevance_scores = sorted(true_rel, reverse=True)\n",
        "  idcg = dcg_at_k(ideal_relevance_scores, k)\n",
        "\n",
        "  return dcg_max/idcg if idcg > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Results**"
      ],
      "metadata": {
        "id": "dq6XzQIvqfcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt5O6tS7wSCw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Main Method: makes calls to all algorithms for bound calculation\n",
        "def calculatePatKandNDCG(file_name, dataInterval, rows, kList, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  # actual top-k\n",
        "  actualTopk, actualTopkDict = calculateActualTopk(file_name)\n",
        "  calculateActualG3Error(file_name, dataInterval, stopAtIndex)\n",
        "  fixedIndexAFDDict = {key: idx for idx, key in enumerate(actualTopkDict.keys())}\n",
        "\n",
        "  # time vs k graphs\n",
        "  lpTimeVsKDict = {}\n",
        "  dcsTimeVsKDict = {}\n",
        "\n",
        "  for k in kList:\n",
        "\n",
        "    # Algorithms 1: Exact LP\n",
        "    startKTimeLP = time.time()\n",
        "    patkGraphLP, ndcgkGraphLP, = [], []\n",
        "\n",
        "    mainUBDictLP, mainLBDictLP, lpTimeVsDataDict = calculateBoundsMLP(file_name, dataInterval, stopAtIndex)\n",
        "    keys = mainUBDictLP.keys()\n",
        "\n",
        "    isTimedLP = False\n",
        "\n",
        "    for key in keys:\n",
        "\n",
        "      ubdataDictLP = mainUBDictLP[key]\n",
        "      lbdataDictLP = mainLBDictLP[key]\n",
        "      attr = list(ubdataDictLP.keys())\n",
        "\n",
        "      exactTopk_mlp = calculateExactTopk(lbdataDictLP, ubdataDictLP)\n",
        "      patkLP = precision_at_k(actualTopk, exactTopk_mlp, k=k)\n",
        "      patkGraphLP.append(patkLP)\n",
        "\n",
        "      actualTopk_rs  = calcRelevanceScore(actualTopkDict, fixedIndexAFDDict, k)\n",
        "      mlpTopk_rs = adjustRelevanceScore(exactTopk_mlp, actualTopk[:k], actualTopk_rs[:k])\n",
        "      ndcgatkLP = ndcg_at_k(mlpTopk_rs, actualTopk_rs, k=k)\n",
        "      ndcgkGraphLP.append(ndcgatkLP)\n",
        "\n",
        "      if not isTimedLP:\n",
        "        if patkLP==1.0 and ndcgatkLP==1.0:\n",
        "          stopKTimeLP = time.time()\n",
        "          lpTimeVsKDict[k] = stopKTimeLP - startKTimeLP\n",
        "          isTimedLP = True\n",
        "\n",
        "    # Algorithms 2: Exact DCS\n",
        "    startKTimeDCS = time.time()\n",
        "    patkGraphDCS, ndcgkGraphDCS, = [], []\n",
        "\n",
        "    mainUBDictDCS, mainLBDictDCS, dcsTimeVsDataDict = calculateBoundsDCS(file_name, dataInterval, stopAtIndex)\n",
        "    keys = mainUBDictDCS.keys()\n",
        "\n",
        "    isTimedDCS = False\n",
        "\n",
        "    for key in keys:\n",
        "\n",
        "      ubdataDictDCS = mainUBDictDCS[key]\n",
        "      lbdataDictDCS = mainLBDictDCS[key]\n",
        "      attr = list(ubdataDictDCS.keys())\n",
        "\n",
        "      exactTopk_dcs = calculateExactTopk(lbdataDictDCS, ubdataDictDCS)\n",
        "      patkDCS = precision_at_k(actualTopk, exactTopk_dcs, k=k)\n",
        "      patkGraphDCS.append(patkDCS)\n",
        "\n",
        "      actualTopk_rs  = calcRelevanceScore(actualTopkDict, fixedIndexAFDDict, k)\n",
        "      dcsTopk_rs = adjustRelevanceScore(exactTopk_dcs, actualTopk[:k], actualTopk_rs[:k])\n",
        "      ndcgatkDCS = ndcg_at_k(dcsTopk_rs, actualTopk_rs, k=k)\n",
        "      ndcgkGraphDCS.append(ndcgatkDCS)\n",
        "\n",
        "      if not isTimedDCS:\n",
        "        if patkDCS==1.0 and ndcgatkDCS==1.0:\n",
        "          stopKTimeDCS = time.time()\n",
        "          dcsTimeVsKDict[k] = stopKTimeDCS - startKTimeDCS\n",
        "          isTimedDCS = True\n",
        "\n",
        "    # p@k and ndcg graphs\n",
        "    generatePatK(patkGraphLP, patkGraphDCS, rows, dataInterval, k, stopAtIndex, fn, sortedStr)\n",
        "    generateNDCGK(ndcgkGraphLP, ndcgkGraphDCS, rows, dataInterval, k, stopAtIndex, fn, sortedStr)\n",
        "\n",
        "  # clocktime graphs\n",
        "  generateKClocktimeGraph(lpTimeVsKDict, dcsTimeVsKDict, kList, fn, sortedStr)\n",
        "  generateDataClocktimeGraph(lpTimeVsDataDict, dcsTimeVsDataDict, rows, dataInterval, stopAtIndex, fn, sortedStr)\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y95GD2Fplnw"
      },
      "source": [
        "**Precision at k Graph**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generatePatK(patkGraph_lb1, patkGraph_lb2, rows, dataInterval, k, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bar_width = 0.25\n",
        "  index = np.arange(len(rows_read))\n",
        "\n",
        "  patterns = ['o', '|']\n",
        "\n",
        "  plt.bar(index, patkGraph_lb1, bar_width, label='ExLP', hatch=patterns[0], facecolor='tan', edgecolor='black')\n",
        "  plt.bar(index + bar_width, patkGraph_lb2, bar_width, label='ExDCS', hatch=patterns[1], facecolor='indianred', edgecolor='black')\n",
        "\n",
        "  plt.xlabel('Percent of Data Read', fontsize=16, fontweight='bold')\n",
        "  plt.ylabel('Precision', fontsize=16, fontweight='bold')\n",
        "\n",
        "  plt.xticks(index + bar_width * 1.75, [f'{round(r, 0)}' for r in rows_read], fontsize=16)\n",
        "  plt.yticks(fontsize=15)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, prop={'weight': 'bold', 'size': 14})\n",
        "\n",
        "  plt.savefig('P@K='+str(k)+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "kwD2yVMJ4ZjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALIH6_nQplnz"
      },
      "source": [
        "**NDCG Graph**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generateNDCGK(ndcgkGraph_lb1, ndcgkGraph_lb2, rows, dataInterval, k, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  bar_width = 0.25\n",
        "  index = np.arange(len(rows_read))\n",
        "\n",
        "  patterns = ['o', '|']\n",
        "\n",
        "  plt.bar(index, ndcgkGraph_lb1, bar_width, label='ExLP', hatch=patterns[0], facecolor='tan', edgecolor='black')\n",
        "  plt.bar(index + bar_width, ndcgkGraph_lb2, bar_width, label='ExDCS', hatch=patterns[1], facecolor='indianred', edgecolor='black')\n",
        "\n",
        "  plt.xlabel('Percent of Data Read', fontsize=16, fontweight='bold')\n",
        "  plt.ylabel('NDCG', fontsize=16, fontweight='bold')\n",
        "\n",
        "  plt.xticks(index + bar_width * 1.75, [f'{round(r, 0)}' for r in rows_read], fontsize=15)\n",
        "  plt.yticks(fontsize=15)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, prop={'weight': 'bold', 'size': 14})\n",
        "\n",
        "  plt.savefig('NDCG@K='+str(k)+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "3ado3m4G5eUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K vs Clocktime Graph**"
      ],
      "metadata": {
        "id": "Ge_VcQLHej_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateKClocktimeGraph(lpTimeVsKDict, dcsTimeVsKDict, kList, fn, sortedStr):\n",
        "\n",
        "  x = kList\n",
        "  y1 = list(lpTimeVsKDict.values())\n",
        "  y2 = list(dcsTimeVsKDict.values())\n",
        "\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  plt.plot(x, y1, marker='v', linestyle='-', color='tan', label='ExLP', linewidth=5)\n",
        "  plt.plot(x, y2, marker='^', linestyle='-', color='indianred', label='ExDCS', linewidth=5)\n",
        "\n",
        "  plt.xticks(x, fontsize=35)\n",
        "  y_min = min(min(y1), min(y2)) - 0.05\n",
        "  y_max = max(max(y1), max(y2)) + 0.05\n",
        "  yticks = np.round(np.linspace(y_min, y_max, num=5), 3)\n",
        "  plt.yticks(yticks, fontsize=35)\n",
        "\n",
        "  plt.xlabel('k', fontsize=34, fontweight='bold')\n",
        "  plt.ylabel('Time (sec)', fontsize=34, fontweight='bold')\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.20), ncol=2, prop={'weight': 'bold', 'size': 35})\n",
        "\n",
        "  plt.savefig('ExKvsClocktime'+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "FC_Id1urah3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Read vs Clocktime Graph**"
      ],
      "metadata": {
        "id": "owpRgfMOmJUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateDataClocktimeGraph(lpdsTimeVsDataDict, ipfTimeVsDataDict, rows, dataInterval, stopAtIndex, fn, sortedStr):\n",
        "\n",
        "  rows_read = list(range(0, round(stopAtIndex) + 1, dataInterval))\n",
        "  rows_read = [round((x / rows) * 100) for x in rows_read]\n",
        "\n",
        "  y1 = list(lpdsTimeVsDataDict.values())\n",
        "  y2 = list(ipfTimeVsDataDict.values())\n",
        "\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  plt.plot(rows_read, y1, marker='v', linestyle='-', color='tan', label='ExLP', linewidth=5)\n",
        "  plt.plot(rows_read, y2, marker='^', linestyle='-', color='indianred', label='ExDCS', linewidth=5)\n",
        "\n",
        "  plt.xticks(rows_read, fontsize=35)\n",
        "  y_min = min(min(y1), min(y2)) - 0.001\n",
        "  y_max = max(max(y1), max(y2)) + 0.001\n",
        "  yticks = np.round(np.linspace(y_min, y_max, num=6), 4)\n",
        "  plt.yticks(yticks, fontsize=35)\n",
        "\n",
        "  plt.xlabel('Data Read (%)', fontsize=34, fontweight='bold')\n",
        "  plt.ylabel('Time (sec)', fontsize=34, fontweight='bold')\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.20), ncol=2, prop={'weight': 'bold', 'size': 35})\n",
        "\n",
        "  plt.savefig('ExDatavsClocktime'+'_'+fn+'_'+sortedStr+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "bKXj7ulJmEq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyaMyjC2MREZ"
      },
      "source": [
        "**Dataset Sorting (Key/Non-Key)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_79xUSXkMQS4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sortDataset(file_name, attr):\n",
        "\n",
        "  df = pd.read_csv(file_name)\n",
        "\n",
        "  if attr != '0':\n",
        "    df_sorted = df.sort_values(by=attr)\n",
        "    sortedFN = 'Sorted_X'+attr+'_'+file_name\n",
        "    df_sorted.to_csv(sortedFN, index=False)\n",
        "  else:\n",
        "    return file_name\n",
        "\n",
        "  return sortedFN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start Point** (Define parameters and run code)"
      ],
      "metadata": {
        "id": "i_A7W6DBwctu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDatasetParam(datasetName):\n",
        "\n",
        "    dataset_config = {\n",
        "        'dataset1': {'fn': 'SyntheticData1', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData1.csv'},\n",
        "        'dataset2': {'fn': 'SyntheticData2', 'rows': 100000, 'interval': 10000, 'fileName': 'SyntheticData2.csv'},\n",
        "        'dataset3': {'fn': 'SyntheticData3', 'rows': 160000, 'interval': 16000, 'fileName': 'SyntheticData3.csv'},\n",
        "        'dataset4': {'fn': 'SyntheticData4', 'rows': 48000, 'interval': 4800, 'fileName': 'SyntheticData4.csv'}\n",
        "    }\n",
        "\n",
        "    if datasetName in dataset_config:\n",
        "        return dataset_config[datasetName]\n",
        "    else:\n",
        "        raise ValueError(f\"{datasetName} not found.\")\n",
        "\n",
        "def AFDStartPoint():\n",
        "\n",
        "  # define dataset paramters\n",
        "  datasetNum = 'dataset1'\n",
        "  params = getDatasetParam(datasetNum)\n",
        "  fn = params['fn']\n",
        "  rows = params['rows']\n",
        "  interval = params['interval']\n",
        "  fileName = params['fileName']\n",
        "\n",
        "  # define sorting order\n",
        "  attr = 'y'\n",
        "  sortedStr = '(Sorted By Z Attribute)'\n",
        "  data = sortDataset(fileName, attr)\n",
        "\n",
        "  # define k values\n",
        "  kList = [3, 5, 7]\n",
        "  # define limit of records read\n",
        "  stopAtIndex = 1*rows\n",
        "\n",
        "  # pass the parameters and generate results\n",
        "  calculatePatKandNDCG(data, interval, rows, kList, stopAtIndex, fn, sortedStr)"
      ],
      "metadata": {
        "id": "ch_1i-bhwilg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start point\n",
        "AFDStartPoint()"
      ],
      "metadata": {
        "id": "h3ve3FNuxm6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}